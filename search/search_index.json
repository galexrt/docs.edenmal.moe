{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Welcome to Eden Docs! This is a documentation of tips, tricks, \"hacks\" and more in multiple different areas. It hasn't been updated in a while, but is still being actively worked on.","title":"Home"},{"location":"#home","text":"Welcome to Eden Docs! This is a documentation of tips, tricks, \"hacks\" and more in multiple different areas. It hasn't been updated in a while, but is still being actively worked on.","title":"Home"},{"location":"databases/MySQL-Family/cheat-sheet/","text":"Re-Create the debian-sys-maint User \u00b6 1 mysqldump --complete-insert --extended-insert = 0 -u root -p mysql | grep 'debian-sys-maint'","title":"Cheat Sheet"},{"location":"databases/MySQL-Family/cheat-sheet/#re-create-the-debian-sys-maint-user","text":"1 mysqldump --complete-insert --extended-insert = 0 -u root -p mysql | grep 'debian-sys-maint'","title":"Re-Create the debian-sys-maint User"},{"location":"databases/MySQL-Family/event_scheduler/","text":"Ever wanted to run queries in a cronjob? Where to safely put the database credentials? MySQL / MariaDB can help out with that. Enable the event_scheduler System \u00b6 You must have the event_scheduler enabled, this can be done by running the following query: 1 SET GLOBAL event_scheduler = ON ; Note It is highly recommended to use the config file(s) of your MySQL / MariaDB server to enable the event_scheduler feature. Another (hacky) way is to use the MySQL server init_file option which runs a SQL script on server startup. Create an Event to run a SQL Query \u00b6 The CREATE EVENT query below would run the query after the DO every day at 02:00 in the exampledb database. 1 2 3 4 5 6 7 8 9 10 CREATE EVENT ` exampledb ` . ` my_cool_table_reset_userOption45 ` ON SCHEDULE EVERY 1 DAY STARTS CURRENT_DATE + INTERVAL 1 DAY + INTERVAL 2 HOUR DO UPDATE ` exampledb ` . ` my_cool_table ` SET userOption45 = '' WHERE userOption45 != '' AND STR_TO_DATE ( userOption45 , '%Y-%m-%d' ) <= NOW (); Show existing Events \u00b6 Note exampledb is the database name. 1 SHOW EVENTS FROM ` exampledb ` ; References \u00b6 MySQL 5.7 Reference - event_scheduler MySQL 5.7 Reference - CREATE EVENT Statement .","title":"event_scheduler"},{"location":"databases/MySQL-Family/event_scheduler/#enable-the-event_scheduler-system","text":"You must have the event_scheduler enabled, this can be done by running the following query: 1 SET GLOBAL event_scheduler = ON ; Note It is highly recommended to use the config file(s) of your MySQL / MariaDB server to enable the event_scheduler feature. Another (hacky) way is to use the MySQL server init_file option which runs a SQL script on server startup.","title":"Enable the event_scheduler System"},{"location":"databases/MySQL-Family/event_scheduler/#create-an-event-to-run-a-sql-query","text":"The CREATE EVENT query below would run the query after the DO every day at 02:00 in the exampledb database. 1 2 3 4 5 6 7 8 9 10 CREATE EVENT ` exampledb ` . ` my_cool_table_reset_userOption45 ` ON SCHEDULE EVERY 1 DAY STARTS CURRENT_DATE + INTERVAL 1 DAY + INTERVAL 2 HOUR DO UPDATE ` exampledb ` . ` my_cool_table ` SET userOption45 = '' WHERE userOption45 != '' AND STR_TO_DATE ( userOption45 , '%Y-%m-%d' ) <= NOW ();","title":"Create an Event to run a SQL Query"},{"location":"databases/MySQL-Family/event_scheduler/#show-existing-events","text":"Note exampledb is the database name. 1 SHOW EVENTS FROM ` exampledb ` ;","title":"Show existing Events"},{"location":"databases/MySQL-Family/event_scheduler/#references","text":"MySQL 5.7 Reference - event_scheduler MySQL 5.7 Reference - CREATE EVENT Statement .","title":"References"},{"location":"databases/MySQL-Family/init_file/","text":"Add the following paramter to the [mysqld] or [mariadb] section of your my.cnf file (depending on the OS, at /etc/mysql/my.cnf , /etc/my.cnf , other path): 1 init_file = /etc/mysql/init.sql The /etc/mysql/init.sql file can contain \"any\" SQL queries. Example to enable / install the MariaDB Query Response Time Plugin plugin : 1 2 3 4 INSTALL SONAME 'query_response_time' ; SET GLOBAL query_response_time_stats = 1 ; SET GLOBAL query_response_time_flush = 1 ; Note There are other ways to install the plugin, but to \"initially\" flush the plugin's data the SET GLOBAL query_response_time_ can be useful to be run. References \u00b6 https://mariadb.com/docs/reference/mdb/system-variables/init_file/ https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_init_file","title":"init_file: Run SQL file on startup"},{"location":"databases/MySQL-Family/init_file/#references","text":"https://mariadb.com/docs/reference/mdb/system-variables/init_file/ https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_init_file","title":"References"},{"location":"databases/MySQL-Family/pitfalls/","text":"mysql CLI Console Password Character \"Limit\" \u00b6 mysql command only reads in 79 characters of \"password\" from TTY / \"console\". In my case I was copy'n'pasting / the password store autotyping database passwords which doesn't work then (leads to Access denied for user errors because the password is wrong). This \"limit\" doesn't seem to be documented anywhere I looked, so here we go. If you want to login to your users only use passwords 79 characters long when \"typing\" them on the console. Thanks to this Jira ticket (MXS-1766) to pointing to the code behind the \"limitation\"! I also need to give a shoutout to the people with which I reflected the issue and then stumbled upon the mysql CLI limitation. Code references : GitHub MariaDB/server - 10.3/client/mysql.cc Line 1959 GitHub MariaDB/server - 10.3/mysys/get_password.c Line 63 Replication User Password Character Limit \u00b6 The password for a replication user must be a maximum of 32 characters long.","title":"Pitfalls"},{"location":"databases/MySQL-Family/pitfalls/#mysql-cli-console-password-character-limit","text":"mysql command only reads in 79 characters of \"password\" from TTY / \"console\". In my case I was copy'n'pasting / the password store autotyping database passwords which doesn't work then (leads to Access denied for user errors because the password is wrong). This \"limit\" doesn't seem to be documented anywhere I looked, so here we go. If you want to login to your users only use passwords 79 characters long when \"typing\" them on the console. Thanks to this Jira ticket (MXS-1766) to pointing to the code behind the \"limitation\"! I also need to give a shoutout to the people with which I reflected the issue and then stumbled upon the mysql CLI limitation. Code references : GitHub MariaDB/server - 10.3/client/mysql.cc Line 1959 GitHub MariaDB/server - 10.3/mysys/get_password.c Line 63","title":"mysql CLI Console Password Character \"Limit\""},{"location":"databases/MySQL-Family/pitfalls/#replication-user-password-character-limit","text":"The password for a replication user must be a maximum of 32 characters long.","title":"Replication User Password Character Limit"},{"location":"general/tools-utilities/","text":"Network \u00b6 DNS \u00b6 https://github.com/DNS-OARC/flamethrower - \"a DNS performance and functional testing utility supporting UDP, TCP, DoT and DoH (by @ns1)\" Kubernetes (K8S) \u00b6 Client \u00b6 https://github.com/kubernetes-sigs/krew/ - Find and install kubectl plugins - krew.dev https://github.com/wercker/stern - Multi pod and container log tailing for Kubernetes with regex support for selection of Pods and Pods' containers. Network \u00b6 https://github.com/inovex/illuminatio - The kubernetes network policy validator. https://github.com/aquasecurity/kube-bench - Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark Monitoring with Prometheus \u00b6 See Monitoring->Prometheus->Exporters page .","title":"Projects, Tools and Utilites"},{"location":"general/tools-utilities/#network","text":"","title":"Network"},{"location":"general/tools-utilities/#dns","text":"https://github.com/DNS-OARC/flamethrower - \"a DNS performance and functional testing utility supporting UDP, TCP, DoT and DoH (by @ns1)\"","title":"DNS"},{"location":"general/tools-utilities/#kubernetes-k8s","text":"","title":"Kubernetes (K8S)"},{"location":"general/tools-utilities/#client","text":"https://github.com/kubernetes-sigs/krew/ - Find and install kubectl plugins - krew.dev https://github.com/wercker/stern - Multi pod and container log tailing for Kubernetes with regex support for selection of Pods and Pods' containers.","title":"Client"},{"location":"general/tools-utilities/#network_1","text":"https://github.com/inovex/illuminatio - The kubernetes network policy validator. https://github.com/aquasecurity/kube-bench - Checks whether Kubernetes is deployed according to security best practices as defined in the CIS Kubernetes Benchmark","title":"Network"},{"location":"general/tools-utilities/#monitoring-with-prometheus","text":"See Monitoring->Prometheus->Exporters page .","title":"Monitoring with Prometheus"},{"location":"general/useful-online-tools/","text":"Diagrams \u00b6 https://app.diagrams.net/ - Previously named draw.io . https://isoflow.io/ - \"Create beautiful cloud diagrams in minutes\" Image Resizing \u00b6 http://waifu2x.udp.jp - Free variant (less zoom). https://mng.waifu2x.me - Paid (not much, billed per minute).","title":"Online Tools"},{"location":"general/useful-online-tools/#diagrams","text":"https://app.diagrams.net/ - Previously named draw.io . https://isoflow.io/ - \"Create beautiful cloud diagrams in minutes\"","title":"Diagrams"},{"location":"general/useful-online-tools/#image-resizing","text":"http://waifu2x.udp.jp - Free variant (less zoom). https://mng.waifu2x.me - Paid (not much, billed per minute).","title":"Image Resizing"},{"location":"kubernetes/cheat-sheet/","text":"Quickly trigger Rolling Update of Deployment, StatefulSet, DaemonSet, etc \u00b6 1 kubectl patch -n kube-system ds kube-proxy -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\" $( date + '%s' ) \\\"}}}}}\" Running kubectl replace / kubectl apply on an object which the command above was used on, will always trigger a rolling update again. This is due to the change to the annotations. Debug Pod manifest to \"escape\" to the node \u00b6 The Pods manifest assumes that you are allowed to run privileged Pods in your cluster. If you are using you may need to set a ServiceAccount which is allowed \"all the things\" (e.g. privileged , hostNetwork , and so on). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kind : Pod apiVersion : v1 metadata : name : debug-pod labels : app : debug spec : hostNetwork : true tolerations : - key : node-role.kubernetes.io/master effect : NoSchedule - key : \"CriticalAddonsOnly\" operator : \"Exists\" restartPolicy : Never hostIPC : true hostPID : true # nodeName: SPECIFIC_TARGET_NODE priorityClassName : \"system-cluster-critical\" containers : - name : debug-pod image : busybox command : [ \"/bin/sleep\" , \"36000\" ] securityContext : privileged : true allowPrivilegeEscalation : true Role Label for Node objects \u00b6 The node-role.kubernetes.io/ can take \"anything\" as a role. Meaning that node-role.kubernetes.io/my-cool-role (any value) will cause the kubectl get nodes output to display my-cool-role (and other such role labels) as the Node role.","title":"Cheat Sheet"},{"location":"kubernetes/cheat-sheet/#quickly-trigger-rolling-update-of-deployment-statefulset-daemonset-etc","text":"1 kubectl patch -n kube-system ds kube-proxy -p \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"annotations\\\":{\\\"date\\\":\\\" $( date + '%s' ) \\\"}}}}}\" Running kubectl replace / kubectl apply on an object which the command above was used on, will always trigger a rolling update again. This is due to the change to the annotations.","title":"Quickly trigger Rolling Update of Deployment, StatefulSet, DaemonSet, etc"},{"location":"kubernetes/cheat-sheet/#debug-pod-manifest-to-escape-to-the-node","text":"The Pods manifest assumes that you are allowed to run privileged Pods in your cluster. If you are using you may need to set a ServiceAccount which is allowed \"all the things\" (e.g. privileged , hostNetwork , and so on). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kind : Pod apiVersion : v1 metadata : name : debug-pod labels : app : debug spec : hostNetwork : true tolerations : - key : node-role.kubernetes.io/master effect : NoSchedule - key : \"CriticalAddonsOnly\" operator : \"Exists\" restartPolicy : Never hostIPC : true hostPID : true # nodeName: SPECIFIC_TARGET_NODE priorityClassName : \"system-cluster-critical\" containers : - name : debug-pod image : busybox command : [ \"/bin/sleep\" , \"36000\" ] securityContext : privileged : true allowPrivilegeEscalation : true","title":"Debug Pod manifest to \"escape\" to the node"},{"location":"kubernetes/cheat-sheet/#role-label-for-node-objects","text":"The node-role.kubernetes.io/ can take \"anything\" as a role. Meaning that node-role.kubernetes.io/my-cool-role (any value) will cause the kubectl get nodes output to display my-cool-role (and other such role labels) as the Node role.","title":"Role Label for Node objects"},{"location":"kubernetes/cluster-components-upgrade-order/","text":"The following is a recommended Upgrade Order for the Components of a Kubernetes cluster: Master Components kube-apiserver kube-controller-manager If used, cloud-controller-manager kube-scheduler Node Components kubelet kube-proxy Other components of a Kubernetes cluster can mostly be updated in any order, as long as the documentation of the component doesn't state otherwise: CNI (e.g., Calico, Cillium) etcd Operators Be aware of potential changes in the operator causing unwanted \"results\".","title":"Cluster Components Upgrade Order"},{"location":"kubernetes/kubeadm/","text":"Coming Soon","title":"kubeadm"},{"location":"kubernetes/recommended-requirement/","text":"{{< panel title=\"Recommendations for Requirements\" icon=\"fas fa-info-circle\" border=\"solid #0000ff 2px\" >}} These are recommendations for requirements of Kubernetes Masters and Nodes. {{ }} General \u00b6 Network: Bandwidth: at the very least 1G, recommended for smaller, lower traffic environents is 10G, 25G or more. Note: To reduce costs you get away with just having a single interface, instead of, e.g., 2 bonded interfaces, though having 2 will allow for more performance and also the server still being reachable in case of failure. Master \u00b6 Storage \u00b6 {{< panel title=\"SSDs or even NVMe based storage is more expensive but your ETCD will love and need it!\" icon=\"fas fa-exclamation-circle\" border=\"solid #ff0000 15px\" >}} DO NOT USE HDDs nor any kind of networked storage for ETCD! Even a fast Ceph RBD (e.g., when running in VMs) can look good in the beginning but might \"kill\" the ETCD performance in the end! Too many users of Kubernetes or OpenShift do that and end up with slow performing clusters in many different ways, simply because the ETCD is slow (even though the kube-apiservers are caching a lot) {{ }} Use SSDs or any other storage with low latencies (LOW)! ETCD is pretty much latency bound, it needs its \"few\" IOPS as well but latency is the killer (sequential writing, e.g., WAL and DB).","title":"Recommended Requirements"},{"location":"kubernetes/recommended-requirement/#general","text":"Network: Bandwidth: at the very least 1G, recommended for smaller, lower traffic environents is 10G, 25G or more. Note: To reduce costs you get away with just having a single interface, instead of, e.g., 2 bonded interfaces, though having 2 will allow for more performance and also the server still being reachable in case of failure.","title":"General"},{"location":"kubernetes/recommended-requirement/#master","text":"","title":"Master"},{"location":"kubernetes/recommended-requirement/#storage","text":"{{< panel title=\"SSDs or even NVMe based storage is more expensive but your ETCD will love and need it!\" icon=\"fas fa-exclamation-circle\" border=\"solid #ff0000 15px\" >}} DO NOT USE HDDs nor any kind of networked storage for ETCD! Even a fast Ceph RBD (e.g., when running in VMs) can look good in the beginning but might \"kill\" the ETCD performance in the end! Too many users of Kubernetes or OpenShift do that and end up with slow performing clusters in many different ways, simply because the ETCD is slow (even though the kube-apiservers are caching a lot) {{ }} Use SSDs or any other storage with low latencies (LOW)! ETCD is pretty much latency bound, it needs its \"few\" IOPS as well but latency is the killer (sequential writing, e.g., WAL and DB).","title":"Storage"},{"location":"kubernetes/tips-and-tricks/","text":"","title":"Tips and Tricks"},{"location":"kubernetes/ETCD/cheat-sheet/","text":"Check ETCD performance \"status\" quickly \u00b6 1 2 3 ETCDCTL_API = 3 etcdctl \\ [ YOUR_FLAGS ] \\ check perf Example \u00b6 Kubernetes ( kubeadm ) \u00b6 1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ check perf Get Metrics from ETCD using curl \u00b6 Note The /etc/etcd/etcd.conf was actively used in OpenShift 3.x installations and some older Kubernetes deployment \"methods\". 1 2 3 4 # Should you still have a `etcd.conf` source it source /etc/etcd/etcd.conf # Otherwise replace each `$ETCD_PEER_*` with the according path curl --cacert = $ETCD_PEER_CA_FILE --cert = $ETCD_PEER_CERT_FILE --key = $ETCD_PEER_KEY_FILE -L https://127.0.0.1:2379/metrics -XGET -v Show ETCD Cluster Members \u00b6 1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ member list","title":"Cheat Sheet"},{"location":"kubernetes/ETCD/cheat-sheet/#check-etcd-performance-status-quickly","text":"1 2 3 ETCDCTL_API = 3 etcdctl \\ [ YOUR_FLAGS ] \\ check perf","title":"Check ETCD performance \"status\" quickly"},{"location":"kubernetes/ETCD/cheat-sheet/#example","text":"","title":"Example"},{"location":"kubernetes/ETCD/cheat-sheet/#kubernetes-kubeadm","text":"1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ check perf","title":"Kubernetes (kubeadm)"},{"location":"kubernetes/ETCD/cheat-sheet/#get-metrics-from-etcd-using-curl","text":"Note The /etc/etcd/etcd.conf was actively used in OpenShift 3.x installations and some older Kubernetes deployment \"methods\". 1 2 3 4 # Should you still have a `etcd.conf` source it source /etc/etcd/etcd.conf # Otherwise replace each `$ETCD_PEER_*` with the according path curl --cacert = $ETCD_PEER_CA_FILE --cert = $ETCD_PEER_CERT_FILE --key = $ETCD_PEER_KEY_FILE -L https://127.0.0.1:2379/metrics -XGET -v","title":"Get Metrics from ETCD using curl"},{"location":"kubernetes/ETCD/cheat-sheet/#show-etcd-cluster-members","text":"1 2 3 4 5 ETCDCTL_API = 3 etcdctl \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ member list","title":"Show ETCD Cluster Members"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/","text":"Danger You should be 100% sure what you are doing and should have at least a snapshot of the etcd you want to edit as things can and will possibly go wrong! Do this at your own risk! Requirements \u00b6 ETCD cluster running. etcdctl can reach it (you need to know which flags to provide, e.g., for tls certs and so on). Golang installed ( glide installed in PATH , e.g., go get -u github.com/Masterminds/glide ). PATH includes the $GOPATH/bin ( export PATH=\"$GOPATH/bin/:$PATH\" ) Steps \u00b6 Step 1 - Prepare Environment \u00b6 1 2 3 4 5 go get -u github.com/jpbetz/auger cd $GOPATH /src/github.com/jpbetz/auger go get -u github.com/Masterminds/glide make vendor # Stay in this directory Warning Be sure to stop all API servers, before continuing with the next steps. Be sure to stop all Controller Manager servers, before continuing with the next steps. Depending on the cluster setup, you may just need move out the according manifest from /etc/kubernetes/manifests directory. Step 2 - Locate object path \u00b6 Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 ETCDCTL_API = 3 etcdctl \\ get /registry/ --keys-only --prefix I recommend you to keep the session on the server for etcdctl open and after finding the correct key to export it using export YOUR_OBJECT_PATH=__PATH__ as it will be used like this later on. Step 3 - Get object from ETCD \u00b6 Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. Replace $YOUR_OBJECT_PATH with the path of the object or set it as a variable. 1 2 3 4 5 6 ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ get $YOUR_OBJECT_PATH > etcd-data-old.bin Copy etcd-data-old.bin to the host, e.g.: 1 scp $SSH_USER @ $SSH_HOST :etcd-data-old.bin . Step 4 - Decode and edit the produced output as you need \u00b6 1 2 cat etcd-data-old.bin | \\ go run main.go decode > object.yaml Now edit the object.yaml as you need. Step 5 - Encode and save data to ETCD \u00b6 1 2 cat object.yaml | \\ go run main.go encode > etcd-data-new.bin Copy the etcd-data-new.bin to the host, e.g.: 1 scp etcd-data-new.bin $SSH_USER @ $SSH_HOST : Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 3 4 5 6 7 cat etcd-data-new.bin | \\ ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ put $YOUR_OBJECT_PATH Step 6 - Verify the object is valid for Kubernetes \u00b6 Just run kubectl get OBJECT_KIND OBJECT_NAME -o yaml on the object you just edited to ensure it is still in working order. If it returns the objects YAML, you are fine. In case of errors, such as illegal bytes or so, you should restore a backup ASAP!","title":"Editing Kubernetes Objects"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#requirements","text":"ETCD cluster running. etcdctl can reach it (you need to know which flags to provide, e.g., for tls certs and so on). Golang installed ( glide installed in PATH , e.g., go get -u github.com/Masterminds/glide ). PATH includes the $GOPATH/bin ( export PATH=\"$GOPATH/bin/:$PATH\" )","title":"Requirements"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#steps","text":"","title":"Steps"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-1-prepare-environment","text":"1 2 3 4 5 go get -u github.com/jpbetz/auger cd $GOPATH /src/github.com/jpbetz/auger go get -u github.com/Masterminds/glide make vendor # Stay in this directory Warning Be sure to stop all API servers, before continuing with the next steps. Be sure to stop all Controller Manager servers, before continuing with the next steps. Depending on the cluster setup, you may just need move out the according manifest from /etc/kubernetes/manifests directory.","title":"Step 1 - Prepare Environment"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-2-locate-object-path","text":"Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 ETCDCTL_API = 3 etcdctl \\ get /registry/ --keys-only --prefix I recommend you to keep the session on the server for etcdctl open and after finding the correct key to export it using export YOUR_OBJECT_PATH=__PATH__ as it will be used like this later on.","title":"Step 2 - Locate object path"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-3-get-object-from-etcd","text":"Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. Replace $YOUR_OBJECT_PATH with the path of the object or set it as a variable. 1 2 3 4 5 6 ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ get $YOUR_OBJECT_PATH > etcd-data-old.bin Copy etcd-data-old.bin to the host, e.g.: 1 scp $SSH_USER @ $SSH_HOST :etcd-data-old.bin .","title":"Step 3 - Get object from ETCD"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-4-decode-and-edit-the-produced-output-as-you-need","text":"1 2 cat etcd-data-old.bin | \\ go run main.go decode > object.yaml Now edit the object.yaml as you need.","title":"Step 4 - Decode and edit the produced output as you need"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-5-encode-and-save-data-to-etcd","text":"1 2 cat object.yaml | \\ go run main.go encode > etcd-data-new.bin Copy the etcd-data-new.bin to the host, e.g.: 1 scp etcd-data-new.bin $SSH_USER @ $SSH_HOST : Note The etcdctl probably needs to be run inside the etcd container on one of the Kubernetes masters. 1 2 3 4 5 6 7 cat etcd-data-new.bin | \\ ETCDCTL_API = 3 etcdctl \\ --endpoints = https:// [ 127 .0.0.1 ] :2379 \\ --cacert = /var/lib/minikube/certs//etcd/ca.crt \\ --cert = /var/lib/minikube/certs//etcd/healthcheck-client.crt \\ --key = /var/lib/minikube/certs//etcd/healthcheck-client.key \\ put $YOUR_OBJECT_PATH","title":"Step 5 - Encode and save data to ETCD"},{"location":"kubernetes/ETCD/editing-kubernetes-objects/#step-6-verify-the-object-is-valid-for-kubernetes","text":"Just run kubectl get OBJECT_KIND OBJECT_NAME -o yaml on the object you just edited to ensure it is still in working order. If it returns the objects YAML, you are fine. In case of errors, such as illegal bytes or so, you should restore a backup ASAP!","title":"Step 6 - Verify the object is valid for Kubernetes"},{"location":"kubernetes/ETCD/snapshots-save-restore/","text":"Warning This page is only for ETCD version 3.x and higher! For the original commands and more information on ETCD, see https://coreos.com/etcd/docs/latest/op-guide/recovery.html . Take a snapshot \u00b6 1 2 3 ETCDCTL_API = 3 etcdctl \\ --endpoints $ETCD_ENDPOINT \\ snapshot save snapshot.db (where snapshot.db is the name of the snapshot file to be created) Restore a snapshot \u00b6 Danger Before restoring a snapshot, all ETCDs in the cluster must be stopped! You must rename/remove the current data dir (probably /var/lib/etcd ). Be sure to provide all flags that are specified in, e.g., systemd unit file, Kubespray: /etc/etcd.env and others otherwise you may create issues for the ETCD cluster! The command is looking about like that depending on what flags are used for your ETCD node: 1 2 3 4 5 6 7 8 9 10 # Run the command as `root` user after that use `chown` to correct ownership of files ETCDCTL_API = 3 etcdctl \\ snapshot restore snapshot.db \\ --name m1 \\ --initial-cluster m1 = http://host1:2380,m2 = http://host2:2380,m3 = http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host1:2380 --data-dir = /var/lib/etcd # chown etcd:etcd -R /var/lib/etcd This has to be done on all ETCD servers one by one with each having their own name given by flag as they were when the snapshot was taken.","title":"Snapshots: Save & Restore"},{"location":"kubernetes/ETCD/snapshots-save-restore/#take-a-snapshot","text":"1 2 3 ETCDCTL_API = 3 etcdctl \\ --endpoints $ETCD_ENDPOINT \\ snapshot save snapshot.db (where snapshot.db is the name of the snapshot file to be created)","title":"Take a snapshot"},{"location":"kubernetes/ETCD/snapshots-save-restore/#restore-a-snapshot","text":"Danger Before restoring a snapshot, all ETCDs in the cluster must be stopped! You must rename/remove the current data dir (probably /var/lib/etcd ). Be sure to provide all flags that are specified in, e.g., systemd unit file, Kubespray: /etc/etcd.env and others otherwise you may create issues for the ETCD cluster! The command is looking about like that depending on what flags are used for your ETCD node: 1 2 3 4 5 6 7 8 9 10 # Run the command as `root` user after that use `chown` to correct ownership of files ETCDCTL_API = 3 etcdctl \\ snapshot restore snapshot.db \\ --name m1 \\ --initial-cluster m1 = http://host1:2380,m2 = http://host2:2380,m3 = http://host3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://host1:2380 --data-dir = /var/lib/etcd # chown etcd:etcd -R /var/lib/etcd This has to be done on all ETCD servers one by one with each having their own name given by flag as they were when the snapshot was taken.","title":"Restore a snapshot"},{"location":"kubernetes/logging/regex/","text":"Some neat regex to parse certain messages from Kubernetes logs. RBAC DENY Messages \u00b6 Good to setup some log alerting on those messages to make sure the applications are not hammering the API servers with \"bad\" RBAC. Match message (should be enough for matching): 1 \\] RBAC DENY: Rewriting into a comma separated list + showing occurence counts: 1 perl -n -e '/\\] RBAC DENY: user \"(.+)\" groups \\[(\".+\")\\] cannot \"([a-zA-Z]+)\" resource \"([a-zA-Z._-]+)\" in namespace \"([a-zA-Z-_]+)\"/ && print \"ns=$5,verb=$3,resource=$4,user=$1,groups=$2\\n\"' | sort | uniq -c","title":"Regex"},{"location":"kubernetes/logging/regex/#rbac-deny-messages","text":"Good to setup some log alerting on those messages to make sure the applications are not hammering the API servers with \"bad\" RBAC. Match message (should be enough for matching): 1 \\] RBAC DENY: Rewriting into a comma separated list + showing occurence counts: 1 perl -n -e '/\\] RBAC DENY: user \"(.+)\" groups \\[(\".+\")\\] cannot \"([a-zA-Z]+)\" resource \"([a-zA-Z._-]+)\" in namespace \"([a-zA-Z-_]+)\"/ && print \"ns=$5,verb=$3,resource=$4,user=$1,groups=$2\\n\"' | sort | uniq -c","title":"RBAC DENY Messages"},{"location":"kubernetes/monitoring/","text":"Use Prometheus for in-cluster monitoring!","title":"Monitoring"},{"location":"kubernetes/monitoring/components/","text":"Infrastructure / Cluster components should be monitored separately from your applications. This allows you to \"kill\" the application Prometheus in case you have screwed up in some way (e.g., messed up application metrics causing to have a billion labeled metrics). Master Components \u00b6 etcd \u00b6 Summary: \"Why do my kubectl commands take so long?\" Port: 2379/TCP Path: /metrics Auth: Client Certificate Notes: Thanks to etcd having a role concept, can be a \"separate\" user with just metrics access. Additionally one might want to run a Kubernetes authenticated OAuth proxy in front of th etcd so that the (one or more) Prometheus can be granted access to it by RoleBinding a Role to the ServiceAccount. E.g., you can create a Secret with the ETCD certificate and key using the kubectl create secret generic --from-file=.../ca.crt --from-file=.../monitoring.crt --from-file=.../monitoring.key Be sure to mount that Secret inside your Prometheus instance and adjust the path(s) according to the mountPath . Prometheus Config Scrape Job Reference, see References Prometheus Kubernetes ETCD Scrape Job Config . What can the Metrics tell us: ETCD disk write latencies (they should be low, very low). ETCD Quorum status as well (+ some other metrics, e.g., how many requests / streams). Golang Process information (e.g., CPU, memory, GC status). kube-apiserver \u00b6 Summary: \"Why do my kubectl commands take so long?\" Port: 6443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: API Request latencies (kubectl, operators, scheduler, controller, etc). Best used with histogram_quantile() Prometheus func over some time. Admission step and webhook controller durations and latencies. ETCD request metrics. API server cache metrics. Kubernetes API Rest Client latency, requests and duration metrics (own requests made). Golang Process information (e.g., CPU, memory, GC status). kube-scheduler \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-scheduler needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: \"How long do my Pods take to be scheudled?\" ( scheduler_binding_* ) Pod Preemption Algorithm latencies and more. Volume scheduling duration. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). kube-controller-manager \u00b6 Info If the cloud controller is used, it should also be monitored / scraped for metrics. Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-controller-manager needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: Work queue item count and duration, and lease holder status. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). kubelet (+ cadvisor) \u00b6 See Node Components - kubelet . kube-proxy \u00b6 See Node Components - kube-proxy . SDN (e.g., Calico, Cilium) \u00b6 See Node Components - kube-proxy . Node Components \u00b6 kubelet (+ \"cadvisor\") \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . kubelet needs to have the following flags active: --authorization-mode=Webhook and --authentication-token-webhook=true . What can the Metrics tell us: kubelet_node_config_error good to know if the latest config works. Kubelet PLEG (\"container runtime status\") metrics. \"Pod Start times\" (use histogram_quantile() ). CGroup metrics of containers (cpu, memory, network) with Namespace and Pod labels. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). kube-proxy \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 10250/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: iptables and / or ipvs sync information (~= how long does it take for Service changes to be reflected in the \"routing\" rules). Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status). SDN / CNI (e.g., Calico, Cilium) \u00b6 Depends on the SDN / CNI used, if there are metrics available. Calico for example can expose metrics, but that must be enabled through a environemnt variable on the Calico Node DaemonSet. For other SDNs, e.g., OpenVSwitch you may need to use an \"external\" exporter when available: https://github.com/digitalocean/openvswitch_exporter https://github.com/ovnworks/ovn_exporter What can the Metrics tell us: Depending on the exporter, at least how much traffic is flowing and / or if there are issues with the daemon. The Nodes themself \u00b6 node_exporter \u00b6 See Monitoring/Prometheus/Exporters - node_exporter . ethtool_exporter \u00b6 See Monitoring/Prometheus/Exporters - ethtool_exporter . Additional In-Cluster Components \u00b6 Other components that are in and / or around a Kubernetes cluster. metrics-server (previously named heapster) \u00b6 (More information https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ ) Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Possibly metrics on how Golang Process information (e.g., CPU, memory, GC status). kube-state-metrics \u00b6 Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 8080/TCP for \"cluster\" metrics and 8081/TCP for kube-state-metrics metrics. (Recommended to scrape both) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Metrics about Deployments, StatefulSets, Jobs, CronJobs, and basically any other objects Status (that is in the official Kubernetes APIs). Golang Process information (e.g., CPU, memory, GC status). Other Components \u00b6 Elasticsearch \u00b6 Elasticsearch is not providing Prometheus metrics itself, but there is a well written exporter GitHub justwatchcom/elasticsearch_exporter . (There are some other exporters also available, though I have used mainly used this one for the amount of metrics I'm able to get from Elasticsearch with it) Summary: \"Is my Elasticsearch able to ingest the amount of logs? Do I need to add more data nodes and / or resources?\" Port: 9114/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Elasticsearch cluster status. Resources (CPU and Memory) and also Storage usage. Elasticsearch Indices status (when enabled (can be filtered in different ways)). Elasticsearch JVM info (GC, memory, etc). Checkout the metrics list for a list of all Metrics available: https://github.com/justwatchcom/elasticsearch_exporter#metrics Golang Process information (e.g., CPU, memory, GC status). Prometheus \u00b6 Summary: \"Does my Prometheus have enough resources? Can I take in another X-thousand / million metrics?\" Port: 9090/TCP (depends on your installation) Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Is my Prometheus doing \"Okay\", e.g., have enough resources Can be used to see if the Prometheus is able to take in another X-thousand / million metrics. Golang Process information (e.g., CPU, memory, GC status). References \u00b6 Prometheus ClusterRole \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : prometheus-infra rules : - apiGroups : - \"\" resources : - nodes/metrics verbs : - get - nonResourceURLs : - /metrics - /metrics/cadvisor verbs : - get Prometheus Per-Namespace Role \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : prometheus namespace : YOUR_NAMESPACE rules : - apiGroups : - \"\" resources : - services - endpoints - pods verbs : - get - list - watch Prometheus Kubernetes ETCD Scrape Job Config \u00b6 Info This selects the master Nodes based on the node-role.kubernetes.io/master label. So be sure to have it set on the master Nodes. For more information see Kubernetes Cheat Sheet - Role Label for Node objects . 1 TODO","title":"Components"},{"location":"kubernetes/monitoring/components/#master-components","text":"","title":"Master Components"},{"location":"kubernetes/monitoring/components/#etcd","text":"Summary: \"Why do my kubectl commands take so long?\" Port: 2379/TCP Path: /metrics Auth: Client Certificate Notes: Thanks to etcd having a role concept, can be a \"separate\" user with just metrics access. Additionally one might want to run a Kubernetes authenticated OAuth proxy in front of th etcd so that the (one or more) Prometheus can be granted access to it by RoleBinding a Role to the ServiceAccount. E.g., you can create a Secret with the ETCD certificate and key using the kubectl create secret generic --from-file=.../ca.crt --from-file=.../monitoring.crt --from-file=.../monitoring.key Be sure to mount that Secret inside your Prometheus instance and adjust the path(s) according to the mountPath . Prometheus Config Scrape Job Reference, see References Prometheus Kubernetes ETCD Scrape Job Config . What can the Metrics tell us: ETCD disk write latencies (they should be low, very low). ETCD Quorum status as well (+ some other metrics, e.g., how many requests / streams). Golang Process information (e.g., CPU, memory, GC status).","title":"etcd"},{"location":"kubernetes/monitoring/components/#kube-apiserver","text":"Summary: \"Why do my kubectl commands take so long?\" Port: 6443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: API Request latencies (kubectl, operators, scheduler, controller, etc). Best used with histogram_quantile() Prometheus func over some time. Admission step and webhook controller durations and latencies. ETCD request metrics. API server cache metrics. Kubernetes API Rest Client latency, requests and duration metrics (own requests made). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-apiserver"},{"location":"kubernetes/monitoring/components/#kube-scheduler","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-scheduler needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: \"How long do my Pods take to be scheudled?\" ( scheduler_binding_* ) Pod Preemption Algorithm latencies and more. Volume scheduling duration. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-scheduler"},{"location":"kubernetes/monitoring/components/#kube-controller-manager","text":"Info If the cloud controller is used, it should also be monitored / scraped for metrics. Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . The kube-controller-manager needs to either listen on :: ( 0.0.0.0 ) or have a proxy which is available to Prometheus for scraping running. What can the Metrics tell us: Work queue item count and duration, and lease holder status. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-controller-manager"},{"location":"kubernetes/monitoring/components/#kubelet-cadvisor","text":"See Node Components - kubelet .","title":"kubelet (+ cadvisor)"},{"location":"kubernetes/monitoring/components/#kube-proxy","text":"See Node Components - kube-proxy .","title":"kube-proxy"},{"location":"kubernetes/monitoring/components/#sdn-eg-calico-cilium","text":"See Node Components - kube-proxy .","title":"SDN (e.g., Calico, Cilium)"},{"location":"kubernetes/monitoring/components/#node-components","text":"","title":"Node Components"},{"location":"kubernetes/monitoring/components/#kubelet-cadvisor_1","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . kubelet needs to have the following flags active: --authorization-mode=Webhook and --authentication-token-webhook=true . What can the Metrics tell us: kubelet_node_config_error good to know if the latest config works. Kubelet PLEG (\"container runtime status\") metrics. \"Pod Start times\" (use histogram_quantile() ). CGroup metrics of containers (cpu, memory, network) with Namespace and Pod labels. Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kubelet (+ \"cadvisor\")"},{"location":"kubernetes/monitoring/components/#kube-proxy_1","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 10250/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: iptables and / or ipvs sync information (~= how long does it take for Service changes to be reflected in the \"routing\" rules). Kubernetes API Rest Client latency, requests and duration metrics (own requests made to the API). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-proxy"},{"location":"kubernetes/monitoring/components/#sdn-cni-eg-calico-cilium","text":"Depends on the SDN / CNI used, if there are metrics available. Calico for example can expose metrics, but that must be enabled through a environemnt variable on the Calico Node DaemonSet. For other SDNs, e.g., OpenVSwitch you may need to use an \"external\" exporter when available: https://github.com/digitalocean/openvswitch_exporter https://github.com/ovnworks/ovn_exporter What can the Metrics tell us: Depending on the exporter, at least how much traffic is flowing and / or if there are issues with the daemon.","title":"SDN / CNI (e.g., Calico, Cilium)"},{"location":"kubernetes/monitoring/components/#the-nodes-themself","text":"","title":"The Nodes themself"},{"location":"kubernetes/monitoring/components/#node_exporter","text":"See Monitoring/Prometheus/Exporters - node_exporter .","title":"node_exporter"},{"location":"kubernetes/monitoring/components/#ethtool_exporter","text":"See Monitoring/Prometheus/Exporters - ethtool_exporter .","title":"ethtool_exporter"},{"location":"kubernetes/monitoring/components/#additional-in-cluster-components","text":"Other components that are in and / or around a Kubernetes cluster.","title":"Additional In-Cluster Components"},{"location":"kubernetes/monitoring/components/#metrics-server-previously-named-heapster","text":"(More information https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/ ) Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 443/TCP Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Possibly metrics on how Golang Process information (e.g., CPU, memory, GC status).","title":"metrics-server (previously named heapster)"},{"location":"kubernetes/monitoring/components/#kube-state-metrics","text":"Summary: \"How long do my Pods take to be scheudled?\" (Hints at \"you need to change your scheduler config\") Port: 8080/TCP for \"cluster\" metrics and 8081/TCP for kube-state-metrics metrics. (Recommended to scrape both) Path: /metrics Auth: ServiceAccount token. Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Metrics about Deployments, StatefulSets, Jobs, CronJobs, and basically any other objects Status (that is in the official Kubernetes APIs). Golang Process information (e.g., CPU, memory, GC status).","title":"kube-state-metrics"},{"location":"kubernetes/monitoring/components/#other-components","text":"","title":"Other Components"},{"location":"kubernetes/monitoring/components/#elasticsearch","text":"Elasticsearch is not providing Prometheus metrics itself, but there is a well written exporter GitHub justwatchcom/elasticsearch_exporter . (There are some other exporters also available, though I have used mainly used this one for the amount of metrics I'm able to get from Elasticsearch with it) Summary: \"Is my Elasticsearch able to ingest the amount of logs? Do I need to add more data nodes and / or resources?\" Port: 9114/TCP (depends on your installation) Path: /metrics Auth: ServiceAccount token Note(s): RBAC ClusterRole needed, see References Prometheus ClusterRole and References Prometheus Per-Namespace Role . What can the Metrics tell us: Elasticsearch cluster status. Resources (CPU and Memory) and also Storage usage. Elasticsearch Indices status (when enabled (can be filtered in different ways)). Elasticsearch JVM info (GC, memory, etc). Checkout the metrics list for a list of all Metrics available: https://github.com/justwatchcom/elasticsearch_exporter#metrics Golang Process information (e.g., CPU, memory, GC status).","title":"Elasticsearch"},{"location":"kubernetes/monitoring/components/#prometheus","text":"Summary: \"Does my Prometheus have enough resources? Can I take in another X-thousand / million metrics?\" Port: 9090/TCP (depends on your installation) Path: /metrics Auth: None. (Recommendation: Add Kubernetes OAuth Proxy in front) What can the Metrics tell us: Is my Prometheus doing \"Okay\", e.g., have enough resources Can be used to see if the Prometheus is able to take in another X-thousand / million metrics. Golang Process information (e.g., CPU, memory, GC status).","title":"Prometheus"},{"location":"kubernetes/monitoring/components/#references","text":"","title":"References"},{"location":"kubernetes/monitoring/components/#prometheus-clusterrole","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : prometheus-infra rules : - apiGroups : - \"\" resources : - nodes/metrics verbs : - get - nonResourceURLs : - /metrics - /metrics/cadvisor verbs : - get","title":"Prometheus ClusterRole"},{"location":"kubernetes/monitoring/components/#prometheus-per-namespace-role","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : prometheus namespace : YOUR_NAMESPACE rules : - apiGroups : - \"\" resources : - services - endpoints - pods verbs : - get - list - watch","title":"Prometheus Per-Namespace Role"},{"location":"kubernetes/monitoring/components/#prometheus-kubernetes-etcd-scrape-job-config","text":"Info This selects the master Nodes based on the node-role.kubernetes.io/master label. So be sure to have it set on the master Nodes. For more information see Kubernetes Cheat Sheet - Role Label for Node objects . 1 TODO","title":"Prometheus Kubernetes ETCD Scrape Job Config"},{"location":"kubernetes/networking/benchmarking/","text":"Network \u00b6 https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf DNS \u00b6 https://github.com/DNS-OARC/flamethrower","title":"Benchmarking"},{"location":"kubernetes/networking/benchmarking/#network","text":"https://github.com/kubernetes/perf-tests/tree/master/network/benchmarks/netperf","title":"Network"},{"location":"kubernetes/networking/benchmarking/#dns","text":"https://github.com/DNS-OARC/flamethrower","title":"DNS"},{"location":"kubernetes/networking/explained/","text":"Assumptions \u00b6 Basic network knowledge (e.g. CIDR, Source and Destination NAT) Basic iptables knowledge Pod/Cluster CIDR: 100.64.0.0/13 Every node gets a /24 podCIDR . Service IP CIDR: 100.72.0.0/16 Node IP CIDR: 10.10.10.0/24 More info on IP Classless Inter-Domain Routing (short CIDR): IPv4 Classless Inter-Domain Routing IPv6 Classless Inter-Domain Routing The source for the diagrams, can be found as .graphml at the same path as the images. Example : kubernetes-networking-explained-network_stack.svg -> kubernetes-networking-explained-network_stack.graphml Network Stack \u00b6 Network Overview caption Traffic Flow \u00b6 Pod to Pod \u00b6 Pod to Pod Traffic Pod to Service IP \u00b6 Pod to Service IP Service IP iptables \u00b6 Service IP iptables NodePort to Service IP to Pod \u00b6 NodePort to Service IP to Pod","title":"Explained"},{"location":"kubernetes/networking/explained/#assumptions","text":"Basic network knowledge (e.g. CIDR, Source and Destination NAT) Basic iptables knowledge Pod/Cluster CIDR: 100.64.0.0/13 Every node gets a /24 podCIDR . Service IP CIDR: 100.72.0.0/16 Node IP CIDR: 10.10.10.0/24 More info on IP Classless Inter-Domain Routing (short CIDR): IPv4 Classless Inter-Domain Routing IPv6 Classless Inter-Domain Routing The source for the diagrams, can be found as .graphml at the same path as the images. Example : kubernetes-networking-explained-network_stack.svg -> kubernetes-networking-explained-network_stack.graphml","title":"Assumptions"},{"location":"kubernetes/networking/explained/#network-stack","text":"Network Overview caption","title":"Network Stack"},{"location":"kubernetes/networking/explained/#traffic-flow","text":"","title":"Traffic Flow"},{"location":"kubernetes/networking/explained/#pod-to-pod","text":"Pod to Pod Traffic","title":"Pod to Pod"},{"location":"kubernetes/networking/explained/#pod-to-service-ip","text":"Pod to Service IP","title":"Pod to Service IP"},{"location":"kubernetes/networking/explained/#service-ip-iptables","text":"Service IP iptables","title":"Service IP iptables"},{"location":"kubernetes/networking/explained/#nodeport-to-service-ip-to-pod","text":"NodePort to Service IP to Pod","title":"NodePort to Service IP to Pod"},{"location":"kubernetes/networking/troubleshooting/","text":"General connectivity test \u00b6 Kuberang is a simple but efficient way to quickly check cluster network connectivity. To quote from the project's README: Quote from GitHub apprenda/kuberang README.md : Has kubectl installed correctly with access controls Has active kubernetes namespace (if specified) Has available workers Has working pod & service networks Has working pod <-> pod DNS Has working master(s) Has the ability to access pods and services from the node you run it on. You can run it: 1 kuberang This will start the test Deployments and Services. DNS Server \u00b6 https://github.com/DNS-OARC/flamethrower","title":"Troubleshooting"},{"location":"kubernetes/networking/troubleshooting/#general-connectivity-test","text":"Kuberang is a simple but efficient way to quickly check cluster network connectivity. To quote from the project's README: Quote from GitHub apprenda/kuberang README.md : Has kubectl installed correctly with access controls Has active kubernetes namespace (if specified) Has available workers Has working pod & service networks Has working pod <-> pod DNS Has working master(s) Has the ability to access pods and services from the node you run it on. You can run it: 1 kuberang This will start the test Deployments and Services.","title":"General connectivity test"},{"location":"kubernetes/networking/troubleshooting/#dns-server","text":"https://github.com/DNS-OARC/flamethrower","title":"DNS Server"},{"location":"linux/mdam/","text":"Note Don't forget to keep your mdadm.conf uptodate when creating, modifiying, deleting mdadm arrays. Generate mdadm.conf \u00b6 1 mdadm --detail --scan >> /etc/mdadm.conf Grow RAID 5 to RAID 6 \u00b6 Danger DON'T FORGET TO SPECIFY THE --backup-file=FILE for mdadm --grow operations! Otherwise if the host is (forced) shutdowned (e.g., power failure), data can / will be lost. (This \"backup file\" should be on different disk / storage, not on the mdadm array you are growing!) Speed up RAID rebuild \u00b6 Note This may or may not improve your mdadm RAID rebuild performance. This assumes your disks are sda , sdb and sdc , and the RAID array is md0 ( /dev/md0 ). 1 2 3 4 5 6 7 8 9 10 11 for disk in sd { a..c } ; do blockdev --setra 16384 \"/dev/ ${ disk } \" echo 1024 > \"/sys/block/ ${ disk } /queue/read_ahead_kb\" echo 256 > \"/sys/block/ ${ disk } /queue/nr_requests\" # Disable NCQ on all disks. echo 1 > \"/sys/block/ ${ disk } /device/queue_depth\" done # Set read-ahead to 64 MiB for /dev/md0 blockdev --setra 65536 /dev/md0 # Set stripe_cache_size to 16 MiB for /dev/md0 echo 16384 > /sys/block/md0/md/stripe_cache_size References \u00b6 mdadm man page","title":"mdadm"},{"location":"linux/mdam/#generate-mdadmconf","text":"1 mdadm --detail --scan >> /etc/mdadm.conf","title":"Generate mdadm.conf"},{"location":"linux/mdam/#grow-raid-5-to-raid-6","text":"Danger DON'T FORGET TO SPECIFY THE --backup-file=FILE for mdadm --grow operations! Otherwise if the host is (forced) shutdowned (e.g., power failure), data can / will be lost. (This \"backup file\" should be on different disk / storage, not on the mdadm array you are growing!)","title":"Grow RAID 5 to RAID 6"},{"location":"linux/mdam/#speed-up-raid-rebuild","text":"Note This may or may not improve your mdadm RAID rebuild performance. This assumes your disks are sda , sdb and sdc , and the RAID array is md0 ( /dev/md0 ). 1 2 3 4 5 6 7 8 9 10 11 for disk in sd { a..c } ; do blockdev --setra 16384 \"/dev/ ${ disk } \" echo 1024 > \"/sys/block/ ${ disk } /queue/read_ahead_kb\" echo 256 > \"/sys/block/ ${ disk } /queue/nr_requests\" # Disable NCQ on all disks. echo 1 > \"/sys/block/ ${ disk } /device/queue_depth\" done # Set read-ahead to 64 MiB for /dev/md0 blockdev --setra 65536 /dev/md0 # Set stripe_cache_size to 16 MiB for /dev/md0 echo 16384 > /sys/block/md0/md/stripe_cache_size","title":"Speed up RAID rebuild"},{"location":"linux/mdam/#references","text":"mdadm man page","title":"References"},{"location":"linux/quick-commands/","text":"Archives \u00b6 Extract all '*.zip' files into a directory named after the zip's filename \u00b6 1 find -name '*.zip' -exec sh -c 'unzip -d \"${1%.*}\" \"$1\"' _ {} \\; Extract all '*.rar' files into a directry named after the rar's filename \u00b6 1 find -name '*.rar' -exec sh -c 'mkdir \"${1%.*}\"; unrar e \"$1\" \"${1%.*}\"' _ {} \\; Extract all '*.7z' files into a directry named after the rar's filename \u00b6 1 find -name '*.7z' -exec sh -c 'mkdir \"${1%.*}\"; 7z x \"$1\" -o\"${1%.*}\"' _ {} \\; Documents \u00b6 Convert PDFs to PNGs (each page is its own image) \u00b6 1 2 3 4 5 for file in *.pdf ; do echo \"Processing file: $file ...\" mkdir -p \" $( basename \" $file \" .pdf ) \" pdftoppm -png \" $file \" \" $( basename \" $file \" .pdf ) /page\" done Run tesseract OCR on all converted Pages \u00b6 1 2 3 4 for file in */*.png ; do echo \"Processing file: $file ...\" tesseract -l deu+eng \" $file \" \" $( echo \" $file \" | sed 's/\\.png$//g' ) \" done Info The -l deu+eng are the languages to use. In this case deu+eng means \"Deutsch\" (German) and \"English\". Convert all '*.docx' files into PDFs (using LibreOffice's lowriter ) \u00b6 1 2 3 4 5 find . -name '*.docx' -print0 | while IFS = read -r -d $'\\0' line ; do echo \"Processing file: $line ...\" lowriter --convert-to pdf \" $line \" --outdir \" $( dirname \" $line \" ) \" done Disks \u00b6 Get UUID for partition \u00b6 1 blkid /dev/sdXY -s UUID -o value Where /dev/sdXY could be, /dev/sda2 , /dev/nvme0n1p1 , and so on.","title":"Quick Commands"},{"location":"linux/quick-commands/#archives","text":"","title":"Archives"},{"location":"linux/quick-commands/#extract-all-zip-files-into-a-directory-named-after-the-zips-filename","text":"1 find -name '*.zip' -exec sh -c 'unzip -d \"${1%.*}\" \"$1\"' _ {} \\;","title":"Extract all '*.zip' files into a directory named after the zip's filename"},{"location":"linux/quick-commands/#extract-all-rar-files-into-a-directry-named-after-the-rars-filename","text":"1 find -name '*.rar' -exec sh -c 'mkdir \"${1%.*}\"; unrar e \"$1\" \"${1%.*}\"' _ {} \\;","title":"Extract all '*.rar' files into a directry named after the rar's filename"},{"location":"linux/quick-commands/#extract-all-7z-files-into-a-directry-named-after-the-rars-filename","text":"1 find -name '*.7z' -exec sh -c 'mkdir \"${1%.*}\"; 7z x \"$1\" -o\"${1%.*}\"' _ {} \\;","title":"Extract all '*.7z' files into a directry named after the rar's filename"},{"location":"linux/quick-commands/#documents","text":"","title":"Documents"},{"location":"linux/quick-commands/#convert-pdfs-to-pngs-each-page-is-its-own-image","text":"1 2 3 4 5 for file in *.pdf ; do echo \"Processing file: $file ...\" mkdir -p \" $( basename \" $file \" .pdf ) \" pdftoppm -png \" $file \" \" $( basename \" $file \" .pdf ) /page\" done","title":"Convert PDFs to PNGs (each page is its own image)"},{"location":"linux/quick-commands/#run-tesseract-ocr-on-all-converted-pages","text":"1 2 3 4 for file in */*.png ; do echo \"Processing file: $file ...\" tesseract -l deu+eng \" $file \" \" $( echo \" $file \" | sed 's/\\.png$//g' ) \" done Info The -l deu+eng are the languages to use. In this case deu+eng means \"Deutsch\" (German) and \"English\".","title":"Run tesseract OCR on all converted Pages"},{"location":"linux/quick-commands/#convert-all-docx-files-into-pdfs-using-libreoffices-lowriter","text":"1 2 3 4 5 find . -name '*.docx' -print0 | while IFS = read -r -d $'\\0' line ; do echo \"Processing file: $line ...\" lowriter --convert-to pdf \" $line \" --outdir \" $( dirname \" $line \" ) \" done","title":"Convert all '*.docx' files into PDFs (using LibreOffice's lowriter)"},{"location":"linux/quick-commands/#disks","text":"","title":"Disks"},{"location":"linux/quick-commands/#get-uuid-for-partition","text":"1 blkid /dev/sdXY -s UUID -o value Where /dev/sdXY could be, /dev/sda2 , /dev/nvme0n1p1 , and so on.","title":"Get UUID for partition"},{"location":"linux/sysctl/","text":"See GitHub Gist 90-edenmal-custom.conf for more information on the used sysctl settings / values. The sysctl can be easily using the following command: 1 2 curl -L gist.githubusercontent.com/galexrt/8faa48a05bab303ec922bd89e8f7adc5/raw/90-edenmal-custom.conf -o /etc/sysctl.d/90-edenmal-custom.conf sysctl --system Info The below list might be outedated, please check the GitHub Gist. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 fs.aio_max_nr = 1048576 fs.file_max = 2097152 fs.inotify.max_user_instances = 5120 fs.inotify.max_user_watches = 1572864 fs.nr_open = 3145728 fs.protected_hardlinks = 1 fs.protected_symlinks = 1 fs.suid_dumpable = 0 kernel.core_uses_pid = 1 kernel.dmesg_restrict = 1 kernel.exec-shield = 1 kernel.kptr_restrict = 1 kernel.yama.ptrace_scope = 2 kernel.panic = 10 kernel.panic_on_oops = 1 kernel.pid_max = 4194303 kernel.randomize_va_space = 2 kernel.sched_autogroup_enabled = 0 kernel.sched_migration_cost = 5000000 kernel.sysrq = 0 net.core.default_qdisc = fq net.core.netdev_budget = 600 net.core.netdev_max_backlog = 65536 net.core.optmem_max = 2048000 net.core.rmem_max = 2048000 net.core.somaxconn = 65536 net.core.wmem_max = 2048000 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.accept_source_route = 0 net.ipv4.conf.all.bootp_relay = 0 net.ipv4.conf.all.forwarding = 1 net.ipv4.conf.all.igmpv2_unsolicited_report_interval = 10000 net.ipv4.conf.all.igmpv3_unsolicited_report_interval = 1000 net.ipv4.conf.all.ignore_routes_with_linkdown = 0 net.ipv4.conf.all.log_martians = 1 net.ipv4.conf.all.proxy_arp = 0 net.ipv4.conf.all.rp_filter = 0 net.ipv4.conf.all.secure_redirects = 1 net.ipv4.conf.all.send_redirects = 0 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.accept_source_route = 0 net.ipv4.conf.default.forwarding = 1 net.ipv4.conf.default.log_martians = 1 net.ipv4.conf.default.rp_filter = 0 net.ipv4.conf.default.secure_redirects = 1 net.ipv4.conf.default.send_redirects = 0 net.ipv4.conf.lo.accept_source_route = 1 net.ipv4.fwmark_reflect = 0 net.ipv4.icmp_echo_ignore_all = 0 net.ipv4.icmp_echo_ignore_broadcasts = 1 net.ipv4.icmp_ignore_bogus_error_responses = 1 net.ipv4.icmp_msgs_burst = 50 net.ipv4.icmp_msgs_per_sec = 1000 net.ipv4.ip_forward = 1 net.ipv4.ipfrag_secret_interval = 600 net.ipv4.ip_local_port_range = 1024 65535 net.ipv4.neigh.default.gc_thresh1 = 4048 net.ipv4.neigh.default.gc_thresh2 = 6144 net.ipv4.neigh.default.gc_thresh3 = 8192 net.ipv4.netfilter.nf_conntrack_generic_timeout = 300 net.ipv4.netfilter.nf_conntrack_tcp_timeout_time_wait = 60 net.ipv4.tcp_abort_on_overflow = 1 net.ipv4.tcp_congestion_control = bbr net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_keepalive_intvl = 25 net.ipv4.tcp_keepalive_probes = 5 net.ipv4.tcp_keepalive_time = 420 net.ipv4.tcp_max_syn_backlog = 4096 net.ipv4.tcp_max_tw_buckets = 160000 net.ipv4.tcp_moderate_rcvbuf = 1 net.ipv4.tcp_no_metrics_save = 1 net.ipv4.tcp_notsent_lowat = 16384 net.ipv4.tcp_rfc1337 = 1 net.ipv4.tcp_rmem = 4096 87380 8388608 net.ipv4.tcp_sack = 1 net.ipv4.tcp_slow_start_after_idle = 0 net.ipv4.tcp_synack_retries = 3 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_syn_retries = 2 net.ipv4.tcp_timestamps = 1 net.ipv4.tcp_tw_recycle = 0 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_wmem = 4096 87380 8388608 net.ipv4.udp_rmem_min = 8192 net.ipv4.udp_wmem_min = 8192 net.ipv4.vs.conntrack = 1 net.ipv4.vs.conn_reuse_mode = 1 net.ipv4.vs.expire_nodest_conn = 1 net.ipv4.vs.sloppy_tcp = 1 net.ipv6.conf.all.accept_ra = 0 net.ipv6.conf.all.accept_ra_defrtr = 0 net.ipv6.conf.all.accept_ra_pinfo = 0 net.ipv6.conf.all.accept_redirects = 0 net.ipv6.conf.all.accept_source_route = 0 net.ipv6.conf.all.forwarding = 1 net.ipv6.conf.default.accept_redirects = 0 net.ipv6.conf.default.accept_source_route = 0 net.ipv6.conf.default.autoconf = 1 net.ipv6.conf.default.forwarding = 1 net.ipv6.conf.default.max_addresses = 16 net.ipv6.ip6frag_secret_interval = 600 net.ipv6.route.max_size = 16384 net.ipv6.xfrm6_gc_thresh = 32768 net.netfilter.nf_conntrack_expect_max = 2048 net.netfilter.nf_conntrack_max = 1024000 net.netfilter.nf_conntrack_tcp_timeout_established = 600 net.nf_conntrack_max = 1024000 vm.overcommit_memory = 1 vm.overcommit_ratio = 20 vm.panic_on_oom = 0","title":"sysctl"},{"location":"linux/GRUB/preparations-for-boot-xyz-file/","text":"Danger Only run the following commands if you know what they are doing! If you already have GRUB installed and working, you probably just need to edit your grub.cfg file (for most OSes in the /boot directory). The disk to be used should be: Boot Target in BIOS / UEFI First in the BIOS / UEFI boot order The disk used in this example is /dev/sda . Prepare Disk partition layout Create two partitions on the disk (use, e.g., fdisk , parted , etc) 1MB (flags: boot ) 10G or more as you want / need (flags: boot ), ext4 formatted Run blkid /dev/sda2 -s UUID -o value to get the UUID of the \"first\" disk's second partition. Save the UUID of the \"first\" disk down. The UUID of the \"first\" disk will be used in form of the __BOOT_PART_UUID__ later on. Mount \"boot\" Partition Mount the second created partition ( /dev/sda2 ): mount /dev/sda2 /boot . Download Fedora vmlinuz , initramfs and install.img to /boot directory. Prefix the downloaded files with fedora- (or whatever you want as long as you change it in the upcoming steps as well) Grub Installation For GRUB2: 1 grub2-install --no-floppy /dev/sda2 For GRUB: 1 grub-install --no-floppy /dev/sda2 Create GRUB boot config file For GRUB2 the path is /boot/grub2/grub.cfg . For GRUB the path is /boot/grub/grub.cfg . Optional steps: Copy your Kickstart and / or config file to the /boot directory This assumes the \"system\" you are using is able to open the /boot mounted partition and read the file from there, e.g., Kickstart can do it like that inst.ks=\"hd:UUID=__BOOT_PART_UUID__:/ks.cfg\" (where the __BOOT_PART_UUID__ is the partition UUID). Reboot and enjoy!","title":"Preparations for 'Boot XYZ FIle'"},{"location":"linux/GRUB/Boot-XYZ-File/img-File/","text":"Info You must have a working GRUB installation already, if not checkout the Preparations for Boot . Example grub.cfg (for Fedora CoreOS installation): 1 2 3 4 5 6 7 8 9 set default = 0 set timeout = 5 # Fedora Kickstart Install menuentry \"Fedora Kickstart Install\" { search --no-floppy --fs-uuid __BOOT_PART_UUID__ --set root # Add aditional kernel command line arguments at the end of the next line linux /fedora-vmlinuz selinux = 0 inst.resolution=800x600 inst.ks=\"hd:UUID=__BOOT_PART_UUID__:/ks.cfg\" inst.stage2=\"hd:UUID=__BOOT_PART_UUID__:/fedora-install.img\" initrd /fedora-initrd.img }","title":".img File"},{"location":"linux/GRUB/Boot-XYZ-File/iso-File/","text":"Info You must have a working GRUB installation already, if not checkout the Preparations for Boot . Why would you need this? When you don't have a KVM or just don't want to use ILO, iDRAC, iPMI or other management tools to mount an ISO. This snippet is from a try to install Fedora CoreOS from an ISO file named install.io from the /boot partition. Example grub.cfg contents ( coreos.inst.install_dev=sda would use the sda device for installation in case of *CoreOS): 1 2 3 4 5 6 7 8 9 10 set default = 0 set timeout = 5 # Fedora Kickstart Install menuentry \"Fedora Kickstart Install\" { search --no-floppy --fs-uuid __BOOT_PART_UUID__ --set root # Booting an ISO loopback loop /install.iso linux (loop)/images/vmlinuz coreos.inst = yes coreos.inst.install_dev=sda coreos.inst.ignition_url=http://example.com/example.ign initrd (loop)/images/initramfs.img } It worked fine though the original case was to load the Ignition file from the boot disk which didn't work, a Webserver / Matchbox server was required to load the Ignition file from.","title":".iso File"},{"location":"logging/loki/","text":"Coming Soon","title":"Loki"},{"location":"monitoring/thanos/","text":"Coming Soon","title":"Thanos"},{"location":"monitoring/prometheus/overview/","text":"","title":"Overview"},{"location":"monitoring/prometheus/tips/","text":"Multiple Exporter on one Server but only one port? \u00b6 (Let's not talk about the reasons, why one only has \"one\" port for such a case..) The following projects can be quite useful to accomplish this: https://github.com/rebuy-de/exporter-merger","title":"Tips"},{"location":"monitoring/prometheus/tips/#multiple-exporter-on-one-server-but-only-one-port","text":"(Let's not talk about the reasons, why one only has \"one\" port for such a case..) The following projects can be quite useful to accomplish this: https://github.com/rebuy-de/exporter-merger","title":"Multiple Exporter on one Server but only one port?"},{"location":"monitoring/prometheus/exporters/dellhw_exporter/","text":"Website / Source Code: https://github.com/galexrt/dellhw_exporter Port: 9137/TCP Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: Dell HW status through the use of Dell's OMSA omreport tool. The dellhw_exporter uses Dell Open Manage Server Administrator omreport to get metrics from Dell hardware. For more info, visit the GitHub repository: https://github.com/galexrt/dellhw_exporter . Info The exporter is written by the project contributors, initially created by Alexander Trost .","title":"dellhw_exporter by galexrt"},{"location":"monitoring/prometheus/exporters/ethtool_exporter/","text":"Info \u00b6 Website / Source Code: https://github.com/Showmax/prometheus-ethtool-exporter Port: 9417/TCP (can also export to prom textilfe format) Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: Interface information like SFP status / information and other interesting network interface details. Exports interface metrics using the ethtool tool. E.g., reports metrics such as temperature, dampening, etc., of SFP interfaces (the SFPs need to have / support Digital Diagnostic Monitoring (DDM) ).","title":"ethtool_exporter by Showmax"},{"location":"monitoring/prometheus/exporters/ethtool_exporter/#info","text":"Website / Source Code: https://github.com/Showmax/prometheus-ethtool-exporter Port: 9417/TCP (can also export to prom textilfe format) Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: Interface information like SFP status / information and other interesting network interface details. Exports interface metrics using the ethtool tool. E.g., reports metrics such as temperature, dampening, etc., of SFP interfaces (the SFPs need to have / support Digital Diagnostic Monitoring (DDM) ).","title":"Info"},{"location":"monitoring/prometheus/exporters/node_exporter/","text":"Website / Source Code: https://github.com/prometheus/node_exporter Port: 9100/TCP Path: /metrics Auth: None. (Recommendation: Add (Kubernetes) OAuth Proxy in front) What can the Metrics tell us: OS metrics (e.g., cpu , loadavg , meminfo and many more). The exporter can export a ton of metrics for Linux based systems. Can export metrics for the following OSes: Darwin, Dragonfly, FreeBSD, Linux, NetBSD, OpenBSD, Solaris. Not all metrics are available for each OS (e.g., only Linux has mdadm metrics). For a list of metrics per OS, see https://github.com/prometheus/node_exporter#enabled-by-default and https://github.com/prometheus/node_exporter#disabled-by-default . Special point about the node_exporter is that it can export metrics from textiles that are in a certain format, see https://github.com/prometheus/node_exporter#textfile-collector . SMART metrics are normally exported like this (e.g., https://github.com/galexrt/docker-node_exporter-smartmon for a container image that runs the smartctl script every X time).","title":"node_exporter by Prometheus Project"},{"location":"monitoring/prometheus/exporters/others/","text":"Checkout the official Prometheus exporters list for many more exporters.","title":"Other exporters"},{"location":"name-schemas/naming-schemas/","text":"Hostname Schema \u00b6 Domain TLD Usage \u00b6 Services: example.services Servers (bare metal, VMs, doesn't matter): example.systems Network hardware: example.network Schema Parts \u00b6 COUNTRY - ISO 3166-1 alpha-3 code, see ISO 3166-1 alpha-3 - Wikipedia and Online Browsing Platform (OBP) - ISO . PROVIDER - 3 character long abbreviation of provider name. DC - Optional info about datacenter/region (e.g., FSN1-DC1 ). CLUSTER - Cluster designation (in case of Kubernetes, should always have k8s in the beginning) and if there can be multiple a number added (with two digits, e.g., 01 , 12 ). ROLE - In case of Kubernetes, e.g., master , etcd , node (other \"special\" roles could be, e.g., ingress , stora (could have a suffix per storage software, e.g. storaceph )). COUNT_OR_ID - A count is a special \"type\". It can for servers that are known to have only a maximum of n machines at maximum, the number of servers padded with zeroes (e.g., max 12 servers results in COUNT for the third machine being 03 ), in case of nodes where there can be an undefined amount of them it should be a shortid. To generate a \"random\" ID ( requires bashids to be installed ): 1 2 3 4 5 bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $ ( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]' Servers / VMs Schema \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # When country, provider and dc should be omited: { CLUSTER } - { ROLE } - { COUNT_OR_ID } .example.systems # Examples: ## Master / Primary Kubernetes node k8s02-master-01.example.systems ## Node / Worker Kubernetes node k8s02-node-4ua16bzb6r7.example.systems # With country, provider and dc in the name: { CLUSTER } - { ROLE } - { COUNT_OR_ID } - { PROVIDER } - { COUNTRY }{ DC } .example.systems # Examples: ## `deu-fsn1dc1` translates to Germany, FSN1-DC14 (Falkenstein) k8s02-master-01-htz-deu-fsn1dc14.example.systems ## `deu-fsn1dc1` translates to Germany, FSN1-DC1 (Falkenstein) k8s02-node-4ua16bzb6r7-htz-deu-fsn1dc1.example.systems ## `eee-west2` translates to GCP Region `europe-west2` k8s02-master-02-gcp-eee-west2.example.systems ## `usa-west1` translates to AWS Region `us-west1` k8s02-node-7ca16bnb2r1-aws-usa-west1.example.systems Script: Gernate Hostname (+ ID) \u00b6 1 2 3 4 5 6 7 8 9 10 CLUSTER = \"k8s02\" ROLE = \"node\" COUNT_OR_ID = \" $( sleep 0 .00001 ; \\ bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]' ) \" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID .example.systems\" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID -htz-deu-fsn1dc1.example.systems\" Note The sleep 0.00001 is used to try to prevent \"duplicates\" when bashids is run in parallel to generate IDs. Services Schema \u00b6 1 2 3 4 5 6 7 8 9 {CLUSTER}-({OWNER}-){ROLE}.example.services # Examples ## Kubernetes cluster \"k8s02\" Loadbalancer owned by the \"system\" k8s02-system-lb.example.services # or the owner can be omitted in such cases k8s02-lb.example.services ## Kubernetes cluster \"k8s02\" hosted TeamSpeak service owned by customer \"gamer\" k8s02-gamer-ts3.example.services","title":"Kubernetes Name Schemas"},{"location":"name-schemas/naming-schemas/#hostname-schema","text":"","title":"Hostname Schema"},{"location":"name-schemas/naming-schemas/#domain-tld-usage","text":"Services: example.services Servers (bare metal, VMs, doesn't matter): example.systems Network hardware: example.network","title":"Domain TLD Usage"},{"location":"name-schemas/naming-schemas/#schema-parts","text":"COUNTRY - ISO 3166-1 alpha-3 code, see ISO 3166-1 alpha-3 - Wikipedia and Online Browsing Platform (OBP) - ISO . PROVIDER - 3 character long abbreviation of provider name. DC - Optional info about datacenter/region (e.g., FSN1-DC1 ). CLUSTER - Cluster designation (in case of Kubernetes, should always have k8s in the beginning) and if there can be multiple a number added (with two digits, e.g., 01 , 12 ). ROLE - In case of Kubernetes, e.g., master , etcd , node (other \"special\" roles could be, e.g., ingress , stora (could have a suffix per storage software, e.g. storaceph )). COUNT_OR_ID - A count is a special \"type\". It can for servers that are known to have only a maximum of n machines at maximum, the number of servers padded with zeroes (e.g., max 12 servers results in COUNT for the third machine being 03 ), in case of nodes where there can be an undefined amount of them it should be a shortid. To generate a \"random\" ID ( requires bashids to be installed ): 1 2 3 4 5 bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $ ( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]'","title":"Schema Parts"},{"location":"name-schemas/naming-schemas/#servers-vms-schema","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # When country, provider and dc should be omited: { CLUSTER } - { ROLE } - { COUNT_OR_ID } .example.systems # Examples: ## Master / Primary Kubernetes node k8s02-master-01.example.systems ## Node / Worker Kubernetes node k8s02-node-4ua16bzb6r7.example.systems # With country, provider and dc in the name: { CLUSTER } - { ROLE } - { COUNT_OR_ID } - { PROVIDER } - { COUNTRY }{ DC } .example.systems # Examples: ## `deu-fsn1dc1` translates to Germany, FSN1-DC14 (Falkenstein) k8s02-master-01-htz-deu-fsn1dc14.example.systems ## `deu-fsn1dc1` translates to Germany, FSN1-DC1 (Falkenstein) k8s02-node-4ua16bzb6r7-htz-deu-fsn1dc1.example.systems ## `eee-west2` translates to GCP Region `europe-west2` k8s02-master-02-gcp-eee-west2.example.systems ## `usa-west1` translates to AWS Region `us-west1` k8s02-node-7ca16bnb2r1-aws-usa-west1.example.systems","title":"Servers / VMs Schema"},{"location":"name-schemas/naming-schemas/#script-gernate-hostname-id","text":"1 2 3 4 5 6 7 8 9 10 CLUSTER = \"k8s02\" ROLE = \"node\" COUNT_OR_ID = \" $( sleep 0 .00001 ; \\ bashids \\ -e \\ -s 'B_r1KMOASvn_5A1hDKCdPXJfrIBcddpwOnT5orXYaQPV4Ixb1zNSpa-nF6HOw8mii3pqovZUtsnGZ5pqbf59wPfeMp9XagGXc8ViJreL_5J1kvSnDCPfqvuV2bmGsx4DrVV_ef3Gr3MgCMrX86TGUjCDeJmM3LONAfKIH_vv0ZR9WWcJJbLCc5xnxWh7Is8qNq95ORIHS6iU4gKZNV-LIxdYxd7WyO2fKeOn8kApv0FFD2ydkJXdz4KjqBEcN5Fu' \\ $( date +%s%6N ) | \\ tr '[:upper:]' '[:lower:]' ) \" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID .example.systems\" echo \" $CLUSTER - $ROLE - $COUNT_OR_ID -htz-deu-fsn1dc1.example.systems\" Note The sleep 0.00001 is used to try to prevent \"duplicates\" when bashids is run in parallel to generate IDs.","title":"Script: Gernate Hostname (+ ID)"},{"location":"name-schemas/naming-schemas/#services-schema","text":"1 2 3 4 5 6 7 8 9 {CLUSTER}-({OWNER}-){ROLE}.example.services # Examples ## Kubernetes cluster \"k8s02\" Loadbalancer owned by the \"system\" k8s02-system-lb.example.services # or the owner can be omitted in such cases k8s02-lb.example.services ## Kubernetes cluster \"k8s02\" hosted TeamSpeak service owned by customer \"gamer\" k8s02-gamer-ts3.example.services","title":"Services Schema"},{"location":"networking/cloudflare/","text":"Create IPv4 and IPv6 IPSets \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 # Create ipsets for IPv4 and IPv6 ipset create cf4 hash:net family inet ipset create cf6 hash:net family inet6 # Create ipset for both lists, so both IP versions can use the same list name ` cf ` ipset create cf list:set cf4 cf6 # Get the current Cloudflare IP lists for ip in $(curl https://www.cloudflare.com/ips-v4); do ipset add cf4 \"$ip\"; done for ip in $(curl https://www.cloudflare.com/ips-v6); do ipset add cf6 \"$ip\"; done Allow 80/tcp (http) and 443/tcp (https) Access to Cloudflare IPs only \u00b6 Note These iptables rules are for a stateful firewall! 1 2 3 4 iptables -A INPUT -m set --match-set cf4 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT iptables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP ip6tables -A INPUT -m set --match-set cf6 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT ip6tables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP","title":"Cloudflare"},{"location":"networking/cloudflare/#create-ipv4-and-ipv6-ipsets","text":"1 2 3 4 5 6 7 8 9 10 11 12 # Create ipsets for IPv4 and IPv6 ipset create cf4 hash:net family inet ipset create cf6 hash:net family inet6 # Create ipset for both lists, so both IP versions can use the same list name ` cf ` ipset create cf list:set cf4 cf6 # Get the current Cloudflare IP lists for ip in $(curl https://www.cloudflare.com/ips-v4); do ipset add cf4 \"$ip\"; done for ip in $(curl https://www.cloudflare.com/ips-v6); do ipset add cf6 \"$ip\"; done","title":"Create IPv4 and IPv6 IPSets"},{"location":"networking/cloudflare/#allow-80tcp-http-and-443tcp-https-access-to-cloudflare-ips-only","text":"Note These iptables rules are for a stateful firewall! 1 2 3 4 iptables -A INPUT -m set --match-set cf4 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT iptables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP ip6tables -A INPUT -m set --match-set cf6 src -p tcp -m multiport --dports http,https -m state --state NEW -j ACCEPT ip6tables -A INPUT -p tcp -m multiport --dports http,https -m state --state NEW -j DROP","title":"Allow 80/tcp (http) and 443/tcp (https) Access to Cloudflare IPs only"},{"location":"networking/cisco/acls/","text":"ACLs \u00b6 1 2 3 4 5 6 7 8 # faculty auf faculty access-list 100 permit tcp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 access-list 100 permit udp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 # Students auf http and https faculty access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 80 access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 443 # Guests block all access access-list 102 deny tcp 172 .17.30.0 0 .0.0.255 any Enter a \"sub\" interface \u00b6 1 ip access-group NUMBER in","title":"ACLs"},{"location":"networking/cisco/acls/#acls","text":"1 2 3 4 5 6 7 8 # faculty auf faculty access-list 100 permit tcp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 access-list 100 permit udp 172 .17.10.0 0 .0.0.255 172 .17.0.0 0 .0.255.255 # Students auf http and https faculty access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 80 access-list 101 permit tcp 172 .17.20.0 0 .0.0.255 host 172 .17.10.2 eq 443 # Guests block all access access-list 102 deny tcp 172 .17.30.0 0 .0.0.255 any","title":"ACLs"},{"location":"networking/cisco/acls/#enter-a-sub-interface","text":"1 ip access-group NUMBER in","title":"Enter a \"sub\" interface"},{"location":"networking/cisco/cheat-sheet/","text":"Allow unsupported Transceivers to be used \u00b6 1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit Quick setup \"cheap\" network \u00b6 More to come here to get a Cisco switch running with \"cheap\" network equipment. 1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit","title":"Cheat Sheet"},{"location":"networking/cisco/cheat-sheet/#allow-unsupported-transceivers-to-be-used","text":"1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit","title":"Allow unsupported Transceivers to be used"},{"location":"networking/cisco/cheat-sheet/#quick-setup-cheap-network","text":"More to come here to get a Cisco switch running with \"cheap\" network equipment. 1 2 3 4 5 enable configure terminal no errdisable detect cause gbic-invalid service unsupported-transceiver exit","title":"Quick setup \"cheap\" network"},{"location":"networking/cisco/switch-configuration/","text":"Basic Commands \u00b6 enable - Privileged mode. configure terminal - Enter global config mode hostname NAME - Set a hostname configure terminal - Enable config mode. no ip domain lookup - Disable accidental DNS lookup (in priv and non priv mode). exit - Go one mode back. Config Mode \u00b6 Enter config mode using configure terminal . line console 0 - Enter line console 0 \"interface\". interface Gi 0/48 - Enter Interface Gigabit 0/48 interface. ip address IP_ADDR SUBNET_MASK - Set IP_ADDR and the SUBNET_MASK for an interface. Make interface dedicated for mgmt \u00b6 1 2 3 4 5 6 7 configure terminal interface Gi 0 /48 no interface port int vlan1 shutdown exit ip default-gateway DEFAULT_GATEWAY Set IP address on interface \u00b6 1 2 3 configure terminal interface Gi 0 /48 ip address IP_ADDR SUBNET_MASK \"Activate\" SSH RSA Key \u00b6 1 crypto key generate rsa general-keys modulus 4096 label sw-azubi-1 Show interface status \u00b6 1 do show interface status Add passwords to console login thingy \u00b6 1 2 3 4 5 6 7 configure terminal line console 0 password YOUR_PASSWORD line vty 0 4 login password YOUR_PASSWORD exit In the global config mode: 1 enable secret YOUR_PASSWORD","title":"Switch Configuration"},{"location":"networking/cisco/switch-configuration/#basic-commands","text":"enable - Privileged mode. configure terminal - Enter global config mode hostname NAME - Set a hostname configure terminal - Enable config mode. no ip domain lookup - Disable accidental DNS lookup (in priv and non priv mode). exit - Go one mode back.","title":"Basic Commands"},{"location":"networking/cisco/switch-configuration/#config-mode","text":"Enter config mode using configure terminal . line console 0 - Enter line console 0 \"interface\". interface Gi 0/48 - Enter Interface Gigabit 0/48 interface. ip address IP_ADDR SUBNET_MASK - Set IP_ADDR and the SUBNET_MASK for an interface.","title":"Config Mode"},{"location":"networking/cisco/switch-configuration/#make-interface-dedicated-for-mgmt","text":"1 2 3 4 5 6 7 configure terminal interface Gi 0 /48 no interface port int vlan1 shutdown exit ip default-gateway DEFAULT_GATEWAY","title":"Make interface dedicated for mgmt"},{"location":"networking/cisco/switch-configuration/#set-ip-address-on-interface","text":"1 2 3 configure terminal interface Gi 0 /48 ip address IP_ADDR SUBNET_MASK","title":"Set IP address on interface"},{"location":"networking/cisco/switch-configuration/#activate-ssh-rsa-key","text":"1 crypto key generate rsa general-keys modulus 4096 label sw-azubi-1","title":"\"Activate\" SSH RSA Key"},{"location":"networking/cisco/switch-configuration/#show-interface-status","text":"1 do show interface status","title":"Show interface status"},{"location":"networking/cisco/switch-configuration/#add-passwords-to-console-login-thingy","text":"1 2 3 4 5 6 7 configure terminal line console 0 password YOUR_PASSWORD line vty 0 4 login password YOUR_PASSWORD exit In the global config mode: 1 enable secret YOUR_PASSWORD","title":"Add passwords to console login thingy"},{"location":"networking/fiber/cheat-sheet/","text":"RJ45 (Copper) T-Base SFP / SFP+ \u00b6 As long as they support one of the T-Base standards (e.g., 1000T-Base), they should be able to be used like a \"normal RJ45 ethernet network\" port. {{< panel title=\"T-Base standard\" icon=\"fas fa-info-circle\" border=\"solid #0000ff 2px\" >}} For the SFP / SFP+ to work with, e.g, 10 / 100 / 1000, each of the nT-Base standards must be \"implemented\" in them. {{ }} Simplex and Multimode fiber cables and transceivers \u00b6 You should not mix simplex with multimode fibers and transceivers, and the other way around. \"You'll never have a second go for APC connectors\" \u00b6 Due to the polishing (angle 8\u00b0) of APC fiber \"ends\", if you plugin the fiber connector the wrong way, you'll \"break\" them (slight crunch noise). E.g., plugging SC APC connectors in the wrong way. Make sure to check the orientation markers and with that always plug APC connectors in the correct way! UPC connectors are having no polish at all so no need to worry about the way to plug them in.","title":"Cheat Sheet"},{"location":"networking/fiber/cheat-sheet/#rj45-copper-t-base-sfp-sfp","text":"As long as they support one of the T-Base standards (e.g., 1000T-Base), they should be able to be used like a \"normal RJ45 ethernet network\" port. {{< panel title=\"T-Base standard\" icon=\"fas fa-info-circle\" border=\"solid #0000ff 2px\" >}} For the SFP / SFP+ to work with, e.g, 10 / 100 / 1000, each of the nT-Base standards must be \"implemented\" in them. {{ }}","title":"RJ45 (Copper) T-Base SFP / SFP+"},{"location":"networking/fiber/cheat-sheet/#simplex-and-multimode-fiber-cables-and-transceivers","text":"You should not mix simplex with multimode fibers and transceivers, and the other way around.","title":"Simplex and Multimode fiber cables and transceivers"},{"location":"networking/fiber/cheat-sheet/#youll-never-have-a-second-go-for-apc-connectors","text":"Due to the polishing (angle 8\u00b0) of APC fiber \"ends\", if you plugin the fiber connector the wrong way, you'll \"break\" them (slight crunch noise). E.g., plugging SC APC connectors in the wrong way. Make sure to check the orientation markers and with that always plug APC connectors in the correct way! UPC connectors are having no polish at all so no need to worry about the way to plug them in.","title":"\"You'll never have a second go for APC connectors\""},{"location":"networking/fiber/glossar/","text":"OS1 and OS2 \u00b6 Mode : Singlemode. Bi-Directional, only one line (Ader?) needed. TODO OM1 / OM2 / OM3 / OM4 / OM5 \u00b6 Mode : Multimode. TODO Simplex vs Duplex fiber \u00b6 Simplex cable are consisiting of a single cable strand. Duplex cable are consisiting of two cable strands (\"glued\" together). Singlemode vs Multimode \u00b6 Singlemode only allows one light mode to pass through at a time. Wikipedia Advantages: Much further distances (e.g., OS2 up to 10 kilometers for 10G). Good for WAN network \"applications\". Disadvantages: Higher cost (depending on where you buy, up to 6-times more expensive). Multimode have shorter reach though are because of their price better suited, for, e.g., in datacenter usage. Wikipedia Advantages: Lower cost (in comparison with Singlemode). Compatible with many different data protocols, e.g., ethernet. Disadvantages: Nowhere near the distance of Singlemode (e.g., OM4 10G distance is \"only\" up to 550 meters).","title":"Glossar"},{"location":"networking/fiber/glossar/#os1-and-os2","text":"Mode : Singlemode. Bi-Directional, only one line (Ader?) needed. TODO","title":"OS1 and OS2"},{"location":"networking/fiber/glossar/#om1-om2-om3-om4-om5","text":"Mode : Multimode. TODO","title":"OM1 / OM2 / OM3 / OM4 / OM5"},{"location":"networking/fiber/glossar/#simplex-vs-duplex-fiber","text":"Simplex cable are consisiting of a single cable strand. Duplex cable are consisiting of two cable strands (\"glued\" together).","title":"Simplex vs Duplex fiber"},{"location":"networking/fiber/glossar/#singlemode-vs-multimode","text":"Singlemode only allows one light mode to pass through at a time. Wikipedia Advantages: Much further distances (e.g., OS2 up to 10 kilometers for 10G). Good for WAN network \"applications\". Disadvantages: Higher cost (depending on where you buy, up to 6-times more expensive). Multimode have shorter reach though are because of their price better suited, for, e.g., in datacenter usage. Wikipedia Advantages: Lower cost (in comparison with Singlemode). Compatible with many different data protocols, e.g., ethernet. Disadvantages: Nowhere near the distance of Singlemode (e.g., OM4 10G distance is \"only\" up to 550 meters).","title":"Singlemode vs Multimode"},{"location":"networking/mikrotik/cheat-sheet/","text":"Quick Run Snippets \u00b6 Automatic OS and Firmware Update \u00b6 1 2 3 4 5 6 7 8 /system routerboard settings set auto-upgrade = yes /system package update check-for-updates once :delay 3s ; :if ( [ get status ] = \"New version is available\" ) do ={ install } :delay 1s: /system routerboard upgrade /system reboot Config \u00b6 Set Hostname / Identity \u00b6 1 2 /system identity set name = HOSTNAME Set Timezone \u00b6 To Europe/Berlin . 1 2 /system clock set time-zone-name = Europe/Berlin Disable Ports beginning with ether \u00b6 1 :foreach i in =[ /interface find name~ \"ether\" ] do ={ /interface ethernet set $i disabled = yes } \"Advertise\" 10G on SFP+ Ports \u00b6 1 :foreach i in =[ /interface find name~ \"sfp-sfpplus\" ] do ={ /interface ethernet set $i advertise = 10000M-full ; } Enable Graphs / Graphing \u00b6 1 2 3 4 5 6 7 /tool graphing set page-refresh = 240 /tool graphing interface add /tool graphing resource # Put your admin / configuration network here add allow-address = 172 .16.0.0/24 Set Boot target of Device \u00b6 1 2 /system routerboard settings set boot-os = router-os","title":"Cheat Sheet"},{"location":"networking/mikrotik/cheat-sheet/#quick-run-snippets","text":"","title":"Quick Run Snippets"},{"location":"networking/mikrotik/cheat-sheet/#automatic-os-and-firmware-update","text":"1 2 3 4 5 6 7 8 /system routerboard settings set auto-upgrade = yes /system package update check-for-updates once :delay 3s ; :if ( [ get status ] = \"New version is available\" ) do ={ install } :delay 1s: /system routerboard upgrade /system reboot","title":"Automatic OS and Firmware Update"},{"location":"networking/mikrotik/cheat-sheet/#config","text":"","title":"Config"},{"location":"networking/mikrotik/cheat-sheet/#set-hostname-identity","text":"1 2 /system identity set name = HOSTNAME","title":"Set Hostname / Identity"},{"location":"networking/mikrotik/cheat-sheet/#set-timezone","text":"To Europe/Berlin . 1 2 /system clock set time-zone-name = Europe/Berlin","title":"Set Timezone"},{"location":"networking/mikrotik/cheat-sheet/#disable-ports-beginning-with-ether","text":"1 :foreach i in =[ /interface find name~ \"ether\" ] do ={ /interface ethernet set $i disabled = yes }","title":"Disable Ports beginning with ether"},{"location":"networking/mikrotik/cheat-sheet/#advertise-10g-on-sfp-ports","text":"1 :foreach i in =[ /interface find name~ \"sfp-sfpplus\" ] do ={ /interface ethernet set $i advertise = 10000M-full ; }","title":"\"Advertise\" 10G on SFP+ Ports"},{"location":"networking/mikrotik/cheat-sheet/#enable-graphs-graphing","text":"1 2 3 4 5 6 7 /tool graphing set page-refresh = 240 /tool graphing interface add /tool graphing resource # Put your admin / configuration network here add allow-address = 172 .16.0.0/24","title":"Enable Graphs / Graphing"},{"location":"networking/mikrotik/cheat-sheet/#set-boot-target-of-device","text":"1 2 /system routerboard settings set boot-os = router-os","title":"Set Boot target of Device"},{"location":"networking/mikrotik/example-configs/","text":"VLANs + VLAN Ingress Filtering \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /interface bridge # DO NOT SET `vlan-filtering=yes` here already! Otherwise you would lock yourself out. add dhcp-snooping = yes frame-types = admit-only-vlan-tagged igmp-snooping = yes ingress-filtering = no name = bridge1 pvid = 4094 vlan-filtering = no /interface vlan add interface = bridge1 name = vlan_10_misc vlan-id = 10 add interface = bridge1 name = vlan_20_seccam vlan-id = 20 add interface = bridge1 name = vlan_30_iot vlan-id = 30 add interface = bridge1 name = vlan_100_guest vlan-id = 100 add interface = bridge1 name = vlan_4093_admin vlan-id = 4093 add interface = bridge1 name = vlan_4094_netmgmt vlan-id = 4094 /interface bridge port add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = bonding1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus2 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus3 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus4 pvid = 10 add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus5 pvid = 4093 add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus6 pvid = 4094 trusted = yes /interface bridge vlan add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 untagged = sfp-sfpplus4 vlan-ids = 10 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 20 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 30 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 100 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 untagged = sfp-sfpplus5 vlan-ids = 4093 add bridge = bridge1 tagged = bonding1,bridge1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 vlan-ids = 4094 # Get an IP for the network management interface /ip dhcp-client add dhcp-options = hostname,clientid disabled = no interface = vlan_4094_netmgmt # Wait for everything to \"settle down\" :delay 3 # Enable VLAN Filtering /interface bridge set bridge1 ingress-filtering = yes vlan-filtering = yes Hardware Offloading ( hw=yes ) \u00b6 It depends on the Router OS version and if the Switch Chip in your MikroTik device supports hardware offloading. See MikroTik Wiki - Manual:Switch Chip Features . Bonding \u00b6 This bonds interfaces sfp-sfpplus7 and sfp-sfpplus8 together as bonding1 interface: 1 2 /interface bonding add lacp-rate = 1sec name = bonding1 slaves = sfp-sfpplus7,sfp-sfpplus8 transmit-hash-policy = layer-2-and-3","title":"Example Configs"},{"location":"networking/mikrotik/example-configs/#vlans-vlan-ingress-filtering","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /interface bridge # DO NOT SET `vlan-filtering=yes` here already! Otherwise you would lock yourself out. add dhcp-snooping = yes frame-types = admit-only-vlan-tagged igmp-snooping = yes ingress-filtering = no name = bridge1 pvid = 4094 vlan-filtering = no /interface vlan add interface = bridge1 name = vlan_10_misc vlan-id = 10 add interface = bridge1 name = vlan_20_seccam vlan-id = 20 add interface = bridge1 name = vlan_30_iot vlan-id = 30 add interface = bridge1 name = vlan_100_guest vlan-id = 100 add interface = bridge1 name = vlan_4093_admin vlan-id = 4093 add interface = bridge1 name = vlan_4094_netmgmt vlan-id = 4094 /interface bridge port add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = bonding1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus1 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus2 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus3 pvid = 4094 trusted = yes add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus4 pvid = 10 add bridge = bridge1 frame-types = admit-only-untagged-and-priority-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus5 pvid = 4093 add bridge = bridge1 frame-types = admit-only-vlan-tagged hw = yes ingress-filtering = yes interface = sfp-sfpplus6 pvid = 4094 trusted = yes /interface bridge vlan add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 untagged = sfp-sfpplus4 vlan-ids = 10 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 20 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 30 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus6 vlan-ids = 100 add bridge = bridge1 tagged = bonding1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 untagged = sfp-sfpplus5 vlan-ids = 4093 add bridge = bridge1 tagged = bonding1,bridge1,sfp-sfpplus1,sfp-sfpplus2,sfp-sfpplus3,sfp-sfpplus6 vlan-ids = 4094 # Get an IP for the network management interface /ip dhcp-client add dhcp-options = hostname,clientid disabled = no interface = vlan_4094_netmgmt # Wait for everything to \"settle down\" :delay 3 # Enable VLAN Filtering /interface bridge set bridge1 ingress-filtering = yes vlan-filtering = yes","title":"VLANs + VLAN Ingress Filtering"},{"location":"networking/mikrotik/example-configs/#hardware-offloading-hwyes","text":"It depends on the Router OS version and if the Switch Chip in your MikroTik device supports hardware offloading. See MikroTik Wiki - Manual:Switch Chip Features .","title":"Hardware Offloading (hw=yes)"},{"location":"networking/mikrotik/example-configs/#bonding","text":"This bonds interfaces sfp-sfpplus7 and sfp-sfpplus8 together as bonding1 interface: 1 2 /interface bonding add lacp-rate = 1sec name = bonding1 slaves = sfp-sfpplus7,sfp-sfpplus8 transmit-hash-policy = layer-2-and-3","title":"Bonding"},{"location":"software/crio/","text":"Default CNI configs cause Cluster Network Issues \u00b6 On most OSes where CRI-O is availabe as a packge, CRI-O comes with some default CNI configs located at /etc/cni/net.d/ . If you, e.g., will be running Kubernetes with a CNI (e.g., Calico, Cilium, Flannel), you must remove those files and restart the CRI-O service. On a new Fedora 32 installation, the following CNI config files needed to be removed and then CRI-O service restarted: \u007f/etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf Tip The CRI-O service must only be restarted if the CRI-O has already been started with those CNI config files in the directory.","title":"CRI-O"},{"location":"software/crio/#default-cni-configs-cause-cluster-network-issues","text":"On most OSes where CRI-O is availabe as a packge, CRI-O comes with some default CNI configs located at /etc/cni/net.d/ . If you, e.g., will be running Kubernetes with a CNI (e.g., Calico, Cilium, Flannel), you must remove those files and restart the CRI-O service. On a new Fedora 32 installation, the following CNI config files needed to be removed and then CRI-O service restarted: \u007f/etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/200-loopback.conf Tip The CRI-O service must only be restarted if the CRI-O has already been started with those CNI config files in the directory.","title":"Default CNI configs cause Cluster Network Issues"},{"location":"software/docker-registry/","text":"Garbage Collection doesn't work with non AWS S3 stores \u00b6 (Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) This is due to GitHub docker/distribution - Issue failed to garbage collect #3200 . Note This workaround / \"fix\" is based on @thomasf (Thomas Fr\u00f6ssman) 's comment in the issue . There is an issue in the s3aws.Walk() function which fails for (most) non AWS S3 storages. To make the garbage collection work, an empty file needs to be created in the bucket at the following path (default S3 bucket settings used in the docker-registry itself) BUCKET_NAME/docker/registry/v2/repositories/ . Info Replace the placeholders ( MC_HOST_CONFIG_NAME , BUCKET_NAME ) according to your docker-registry S3 storage configuration. Using the Minio client mc (compatible with most / all S3 based storages) the following command should \"fix\" the issue: 1 2 3 4 5 6 # Create empty file touch workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 # Upload the empty file mc cp workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/ # Verify that the file has been uploaded mc ls MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/ Configuration through Environment Variables Fails \u00b6 Sometimes when \"deeply nesting\" environment variables to configure a certain aspect of the docker-registry configuration, e.g., REGISTRY_STORAGE_MAINTENANCE_READONLY_ENABLED , need to be specified as YAML or JSON starting from \"a few levels\" further below of the config structure: 1 2 3 REGISTRY_STORAGE_MAINTENANCE : |- uploadpurging: enabled: false This has been posted for a similar config situation by @0rax in GitHub docker/distribution - Registry - Upload purging environment overrides crash registry at startup Issue #1736 .","title":"Docker Registry"},{"location":"software/docker-registry/#garbage-collection-doesnt-work-with-non-aws-s3-stores","text":"(Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) This is due to GitHub docker/distribution - Issue failed to garbage collect #3200 . Note This workaround / \"fix\" is based on @thomasf (Thomas Fr\u00f6ssman) 's comment in the issue . There is an issue in the s3aws.Walk() function which fails for (most) non AWS S3 storages. To make the garbage collection work, an empty file needs to be created in the bucket at the following path (default S3 bucket settings used in the docker-registry itself) BUCKET_NAME/docker/registry/v2/repositories/ . Info Replace the placeholders ( MC_HOST_CONFIG_NAME , BUCKET_NAME ) according to your docker-registry S3 storage configuration. Using the Minio client mc (compatible with most / all S3 based storages) the following command should \"fix\" the issue: 1 2 3 4 5 6 # Create empty file touch workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 # Upload the empty file mc cp workaround_s3aws_walk_issue_github_docker_distribtuion_issue_3200 MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/ # Verify that the file has been uploaded mc ls MC_HOST_CONFIG_NAME/BUCKET_NAME/docker/registry/v2/repositories/","title":"Garbage Collection doesn't work with non AWS S3 stores"},{"location":"software/docker-registry/#configuration-through-environment-variables-fails","text":"Sometimes when \"deeply nesting\" environment variables to configure a certain aspect of the docker-registry configuration, e.g., REGISTRY_STORAGE_MAINTENANCE_READONLY_ENABLED , need to be specified as YAML or JSON starting from \"a few levels\" further below of the config structure: 1 2 3 REGISTRY_STORAGE_MAINTENANCE : |- uploadpurging: enabled: false This has been posted for a similar config situation by @0rax in GitHub docker/distribution - Registry - Upload purging environment overrides crash registry at startup Issue #1736 .","title":"Configuration through Environment Variables Fails"},{"location":"software/harbor-registry/","text":"Garbage Collection (GC) not working with non-AWS S3 storage? \u00b6 (Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) See Docker Registry - Garbage Collection doesn't work with non AWS S3 stores . Notes \u00b6 [Security] : Images are not scanned on push by default. This option must be enabled per project / group as of today, 28.08.2020. [Kubernetes] : The Job Service Deployment 's PersistentVolumeClaim must be of type ReadWriteMany . Otherwise having more than replicas: 1 will not work!","title":"Harbor Registry"},{"location":"software/harbor-registry/#garbage-collection-gc-not-working-with-non-aws-s3-storage","text":"(Non AWS S3 stores, e.g., Ceph RGW, Minio, Linode Object store and other similar stores) See Docker Registry - Garbage Collection doesn't work with non AWS S3 stores .","title":"Garbage Collection (GC) not working with non-AWS S3 storage?"},{"location":"software/harbor-registry/#notes","text":"[Security] : Images are not scanned on push by default. This option must be enabled per project / group as of today, 28.08.2020. [Kubernetes] : The Job Service Deployment 's PersistentVolumeClaim must be of type ReadWriteMany . Otherwise having more than replicas: 1 will not work!","title":"Notes"},{"location":"storage/NFS/common-issues/","text":"PHP Applications hanging / timing out (e.g., Nextcloud) \u00b6 In case of Nextcloud on NFS, the application was stuck / hanging at a flock syscall. To \"workaround\" the issue, if applicable for the application (should be for most PHP applications), the NFS must be mounted with the nolock mount option added.","title":"Common Issues"},{"location":"storage/NFS/common-issues/#php-applications-hanging-timing-out-eg-nextcloud","text":"In case of Nextcloud on NFS, the application was stuck / hanging at a flock syscall. To \"workaround\" the issue, if applicable for the application (should be for most PHP applications), the NFS must be mounted with the nolock mount option added.","title":"PHP Applications hanging / timing out (e.g., Nextcloud)"},{"location":"storage/ceph/architecture/","text":"The source for the diagrams, can be found as .graphml at the same path as the images. Basic Cluster with HDDs and SSDs \u00b6 Cluster with RGW for S3-compatible Object Storage \u00b6 No direct OSD access network is required by the consumers of the object storage. (^ the big advantage over CephFS) Cluster with MDS CephFS \u00b6 A filesystem consumer must have direct (/ \"full\") network to the OSDs. Cluster with NVMe OSDs (+ Multi Datacenter Scenario) \u00b6","title":"Architecture"},{"location":"storage/ceph/architecture/#basic-cluster-with-hdds-and-ssds","text":"","title":"Basic Cluster with HDDs and SSDs"},{"location":"storage/ceph/architecture/#cluster-with-rgw-for-s3-compatible-object-storage","text":"No direct OSD access network is required by the consumers of the object storage. (^ the big advantage over CephFS)","title":"Cluster with RGW for S3-compatible Object Storage"},{"location":"storage/ceph/architecture/#cluster-with-mds-cephfs","text":"A filesystem consumer must have direct (/ \"full\") network to the OSDs.","title":"Cluster with MDS CephFS"},{"location":"storage/ceph/architecture/#cluster-with-nvme-osds-multi-datacenter-scenario","text":"","title":"Cluster with NVMe OSDs (+ Multi Datacenter Scenario)"},{"location":"storage/ceph/common-issues/","text":"CephFS mount issues on Hosts \u00b6 Make sure you have a (active) Linux kernel of version 4.17 or higher. Tip In general it is recommended to have a very up-to-date version of the Linux kernel, as many improvements have been made to the Ceph kernel drivers in newer kernel versions ( 5.x or higher). HEALTH_WARN 1 large omap objects \u00b6 Issue \u00b6 1 2 3 HEALTH_WARN 1 large omap objects # and/or LARGE_OMAP_OBJECTS 1 large omap objects Solution \u00b6 The following command should fix the issue: 1 radosgw-admin reshard stale-instances rm","title":"Common Issues"},{"location":"storage/ceph/common-issues/#cephfs-mount-issues-on-hosts","text":"Make sure you have a (active) Linux kernel of version 4.17 or higher. Tip In general it is recommended to have a very up-to-date version of the Linux kernel, as many improvements have been made to the Ceph kernel drivers in newer kernel versions ( 5.x or higher).","title":"CephFS mount issues on Hosts"},{"location":"storage/ceph/common-issues/#health_warn-1-large-omap-objects","text":"","title":"HEALTH_WARN 1 large omap objects"},{"location":"storage/ceph/common-issues/#issue","text":"1 2 3 HEALTH_WARN 1 large omap objects # and/or LARGE_OMAP_OBJECTS 1 large omap objects","title":"Issue"},{"location":"storage/ceph/common-issues/#solution","text":"The following command should fix the issue: 1 radosgw-admin reshard stale-instances rm","title":"Solution"},{"location":"storage/ceph/osds/","text":"OSD Maintenance \u00b6 Gracefully remove OSD \u00b6 Tip If you are using Rook Ceph Operator to run a Ceph cluster in Kubernetes, please follow the official documentation here: Rook Docs - Ceph OSD Management . First thing is to set the crush weight to zero, either instantly to 0.0 or a bit gracefully . ( gracefully should always be used when the cluster is in use, though any OSD weight change will cause data redistribution) 1 ceph osd crush reweight osd.<ID> 0 .0 or graceful: 1 2 3 4 5 for i in { 9 1 } ; do ceph osd crush reweight osd.<ID> 0 . $i # Wait five minutes each step or longer depending on your Ceph cluster recovery speed sleep 300 done After the reweight, set the OSD out and remove it (+ its credentials): 1 ceph osd out <ID> 1 2 3 ceph osd crush remove osd.<ID> ceph auth del osd.<ID> ceph osd rm <ID>","title":"OSDs"},{"location":"storage/ceph/osds/#osd-maintenance","text":"","title":"OSD Maintenance"},{"location":"storage/ceph/osds/#gracefully-remove-osd","text":"Tip If you are using Rook Ceph Operator to run a Ceph cluster in Kubernetes, please follow the official documentation here: Rook Docs - Ceph OSD Management . First thing is to set the crush weight to zero, either instantly to 0.0 or a bit gracefully . ( gracefully should always be used when the cluster is in use, though any OSD weight change will cause data redistribution) 1 ceph osd crush reweight osd.<ID> 0 .0 or graceful: 1 2 3 4 5 for i in { 9 1 } ; do ceph osd crush reweight osd.<ID> 0 . $i # Wait five minutes each step or longer depending on your Ceph cluster recovery speed sleep 300 done After the reweight, set the OSD out and remove it (+ its credentials): 1 ceph osd out <ID> 1 2 3 ceph osd crush remove osd.<ID> ceph auth del osd.<ID> ceph osd rm <ID>","title":"Gracefully remove OSD"},{"location":"storage/gluster/common-issues/","text":"No File Locking \u00b6 Note The described case here is a define \"no-go\" to run on a GlusterFS, but it is worth to mention as some other applications (possibly git and others) might have issues in the long run as well. GlusterFS doesn't seem to have file locking, meaning that, e.g., a SQLite database will corrupt if multiple hosts try to access it.","title":"Common Issues"},{"location":"storage/gluster/common-issues/#no-file-locking","text":"Note The described case here is a define \"no-go\" to run on a GlusterFS, but it is worth to mention as some other applications (possibly git and others) might have issues in the long run as well. GlusterFS doesn't seem to have file locking, meaning that, e.g., a SQLite database will corrupt if multiple hosts try to access it.","title":"No File Locking"},{"location":"storage/rook/architecture/","text":"The source for the diagrams, can be found as .graphml at the same path as the images. Rook Ceph Operator with \"basic\" components running \u00b6 Where the \"basic\" components are the rook-ceph-agent and rook-discover DaemonSet.","title":"Architecture"},{"location":"storage/rook/architecture/#rook-ceph-operator-with-basic-components-running","text":"Where the \"basic\" components are the rook-ceph-agent and rook-discover DaemonSet.","title":"Rook Ceph Operator with \"basic\" components running"},{"location":"storage/rook/common-issues/","text":"Be sure to checkout the Rook Ceph Common Issues page and that all prerequisites for Ceph (and Rook) are met.","title":"Common Issues"}]}